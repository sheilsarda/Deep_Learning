{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import display # to display images\n",
    "import torchvision as thv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403,
     "referenced_widgets": [
      "4b4e36eaa67a40b7b22efe476f4e15e4",
      "c7f0212d040a471695d590b178f03241",
      "0fd3eb64b0f749d28b491411fbfcf223",
      "091f4f0d8c8441b6976e75cebfb17053",
      "83b23b4ce5b64989bb72031cc32d71a0",
      "48e9fc38c7464d44bb4ff1940fbf759b",
      "70083abbfaa8407281bce3a913b08cdc",
      "b2c6238a17dd4fce874cbe649279b97e",
      "3916ae51fb1b4c77921615fa24cdf32d",
      "f6415aae2a164cc08d6858f40a721998",
      "dcc2b4de78cd473e81b3d6e55f48d553",
      "d05641471df14e6bb47fb1acf83ce3df",
      "1126606b8f4547f8acc4230ba67b2533",
      "2c0f85f047cc4f6ca5278f56ebc5aa83",
      "4102a231a4a14472b36aa874449dd1c4",
      "55c3c70f41214420b894682f6be19423",
      "90b8bc0cff954be7a66b33e9f164384b",
      "734fddbcdee948b2a5c4b4229346df00",
      "4b35723c14cc403bb37c3347df7d186b",
      "787cda2a8eb14580b195e8f01fcad7bd",
      "95475e766abb4b1aa97f9a03235480b6",
      "414cea93d76a4e388b01c54b41e88b26",
      "f8b38cc498c54891a3dfec2170156f1d",
      "6c3d26ce1110446ea7819cd0b32a80e4",
      "4556ca81eb19480eb7f324a3f14b39a2",
      "8ed71dc6c5864a7ca7222f69ce29f2b4",
      "a30aa54a3a134dd683d06e4aec47e9d8",
      "3388a777d42c4d69b4700885f23f5e95",
      "8574279e012a459aa029dd5d5cea6975",
      "f072fb0b384b49b1968d98a8d9bd0d54",
      "bd461c4d38fe497384496e50c401a032",
      "d717556978014ebc83542ba66777cd59"
     ]
    },
    "id": "Uh0PPHsrWceW",
    "outputId": "5141d7e6-674e-4315-e61b-4d0bfbc298a0"
   },
   "outputs": [],
   "source": [
    "def shuffle_dataset(X, Y):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "\n",
    "    return X[indices], Y[indices]\n",
    "\n",
    "def subsample(dataset, num_samples, num_classes):\n",
    "    X = dataset.data.numpy()\n",
    "\n",
    "    # flatten X and change type\n",
    "    X = X.reshape(X.shape[0], -1).astype(np.float32)\n",
    "\n",
    "    Y = np.array(dataset.targets)\n",
    "\n",
    "    # final size of each class\n",
    "    class_size = int(num_samples / 10)\n",
    "    indices = []\n",
    "\n",
    "    for label in range(num_classes):\n",
    "        # find indices for the given label and select the first class_size elements\n",
    "        label_indices = np.argwhere(Y == label)[:class_size].flatten()\n",
    "        indices.extend(label_indices)\n",
    "\n",
    "    indices = np.array(indices)\n",
    "    sampled_X = X[indices]\n",
    "    sampled_Y = Y[indices]\n",
    "\n",
    "    # normalize x\n",
    "    sampled_X = sampled_X / 255.0\n",
    "\n",
    "    return sampled_X, sampled_Y\n",
    "\n",
    "def batch_generator(X, Y, batch_size):\n",
    "    while True:\n",
    "        X, Y = shuffle_dataset(X, Y)\n",
    "        for i in range((X.shape[0] - batch_size) // batch_size):\n",
    "            yield X[i * batch_size: i * batch_size + batch_size], Y[i * batch_size: i * batch_size + batch_size]\n",
    "\n",
    "np.random.seed(20)\n",
    "\n",
    "# load dataset\n",
    "train = thv.datasets.MNIST('./', download=True, train=True)\n",
    "val = thv.datasets.MNIST('./', download=True, train=False)\n",
    "\n",
    "trainX, trainY = subsample(dataset=train, num_samples=10*1000, num_classes=10)\n",
    "valX, valY = subsample(dataset=val, num_samples=500*10, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_images(df, old, new):\n",
    "    df_temp = df\n",
    "    num_samples = df_temp.shape[0]\n",
    "    df_temp = df_temp.reshape((num_samples, old, old))\n",
    "    downsized = np.zeros((num_samples, new, new))\n",
    "    for ix in range(num_samples):\n",
    "        downsized[ix] = cv2.resize(df_temp[ix], (new, new))\n",
    "        _, downsized[ix] = cv2.threshold(downsized[ix]*255,127,255,cv2.THRESH_BINARY)\n",
    "    downsized = downsized.reshape((num_samples, new**2))\n",
    "    return downsized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_train_x = resize_images(trainX, 28, 14)\n",
    "resized_val_x = resize_images(valX, 28, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "w_bmE5CdSB0c",
    "outputId": "4d40b500-cdb1-4e7d-b6bc-f4d475fe2c45"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 4)\n",
    "index = np.random.randint(len(trainX), size=8)\n",
    "i = 0\n",
    "\n",
    "for r in range(2):\n",
    "    for c in range(4):\n",
    "        axs[r, c].imshow(resized_train_x[index[i]].reshape(14, 14))\n",
    "        axs[r, c].set_title(str(trainY[index[i]]))\n",
    "        axs[r, c].axis('off')\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403,
     "referenced_widgets": [
      "4b4e36eaa67a40b7b22efe476f4e15e4",
      "c7f0212d040a471695d590b178f03241",
      "0fd3eb64b0f749d28b491411fbfcf223",
      "091f4f0d8c8441b6976e75cebfb17053",
      "83b23b4ce5b64989bb72031cc32d71a0",
      "48e9fc38c7464d44bb4ff1940fbf759b",
      "70083abbfaa8407281bce3a913b08cdc",
      "b2c6238a17dd4fce874cbe649279b97e",
      "3916ae51fb1b4c77921615fa24cdf32d",
      "f6415aae2a164cc08d6858f40a721998",
      "dcc2b4de78cd473e81b3d6e55f48d553",
      "d05641471df14e6bb47fb1acf83ce3df",
      "1126606b8f4547f8acc4230ba67b2533",
      "2c0f85f047cc4f6ca5278f56ebc5aa83",
      "4102a231a4a14472b36aa874449dd1c4",
      "55c3c70f41214420b894682f6be19423",
      "90b8bc0cff954be7a66b33e9f164384b",
      "734fddbcdee948b2a5c4b4229346df00",
      "4b35723c14cc403bb37c3347df7d186b",
      "787cda2a8eb14580b195e8f01fcad7bd",
      "95475e766abb4b1aa97f9a03235480b6",
      "414cea93d76a4e388b01c54b41e88b26",
      "f8b38cc498c54891a3dfec2170156f1d",
      "6c3d26ce1110446ea7819cd0b32a80e4",
      "4556ca81eb19480eb7f324a3f14b39a2",
      "8ed71dc6c5864a7ca7222f69ce29f2b4",
      "a30aa54a3a134dd683d06e4aec47e9d8",
      "3388a777d42c4d69b4700885f23f5e95",
      "8574279e012a459aa029dd5d5cea6975",
      "f072fb0b384b49b1968d98a8d9bd0d54",
      "bd461c4d38fe497384496e50c401a032",
      "d717556978014ebc83542ba66777cd59"
     ]
    },
    "id": "Uh0PPHsrWceW",
    "outputId": "5141d7e6-674e-4315-e61b-4d0bfbc298a0"
   },
   "outputs": [],
   "source": [
    "train_dataloader = batch_generator(resized_train_x, trainY, batch_size=64)\n",
    "val_dataloader = batch_generator(resized_val_x, valY, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Two fully-connected layers\n",
    "    - first has 196 inputs and 128 outputs + tanh nonlinearity\n",
    "    - second has 128 inputs and 16 outputs + no nonlinearity\n",
    "1. Decoder takes as an input z, pushes it through:\n",
    "    - one layer with 128 outputs + tanh nonlinearity\n",
    "    - another layer with 196 output neurons + sigmoid nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL(mu, sigma):\n",
    "    runningKL = 0\n",
    "    for i in range(8):\n",
    "        contribution = 1 + torch.log(sigma[i]**2) - mu[i]**2 - sigma[i]**2\n",
    "        runningKL += -torch.sum(contribution)/2\n",
    "    return runningKL/mu.shape[0]\n",
    "\n",
    "def loss(x, mu, sigma, y):\n",
    "#     print(\"x \", x)\n",
    "#     print(\"y \", y)\n",
    "    kl_loss = KL(mu, sigma)\n",
    "    bce = nn.BCELoss()(y, x)\n",
    "    print('kl: ', kl_loss, 'bce: ', bce)\n",
    "    return kl_loss + bce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss(x, mu, sigma, y):\n",
    "#     # how well do input x and output recon_x agree?\n",
    "#     BCE = nn.BCELoss()(y, x.reshape((x.shape[0], -1)))\n",
    "\n",
    "#     # - D_{KL} = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "#     KLD = 0\n",
    "#     for i in range(8):\n",
    "#         KLD += torch.sum((1 + torch.log(sigma[i]**2) - mu[i]**2 - sigma[i]**2)) / (-2)\n",
    "    \n",
    "#     # Normalise by same number of elements as in reconstruction\n",
    "#     KLD /= mu.shape[0]\n",
    "\n",
    "#     # BCE tries to make our reconstruction as accurate as possible\n",
    "#     # KLD tries to push the distributions as close as possible to unit Gaussian  \n",
    "#     return BCE + KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(196, 128)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "\n",
    "    def forward(self, x):\n",
    "        reshaped = x.reshape(x.shape[0], -1)\n",
    "        out = torch.tanh(self.fc1(reshaped))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.fc1 = nn.Linear(8, 128)\n",
    "        self.fc2 = nn.Linear(128, 196)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.tanh(self.fc1(x))\n",
    "        out = torch.sigmoid(self.fc2(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "encoder = Encoder()\n",
    "encoder_optimzation = optim.Adam(enc.parameters(), lr = lr)\n",
    "\n",
    "decoder = Decoder()\n",
    "decoder_optimzation = optim.SGD(dec.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_list, train_loss_list, val_error_list, val_loss_list = [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "w_bmE5CdSB0c",
    "outputId": "4d40b500-cdb1-4e7d-b6bc-f4d475fe2c45",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kl:  tensor(1.0931, grad_fn=<DivBackward0>) bce:  tensor(-0.1379, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "0 0.9551087617874146\n",
      "kl:  tensor(1.0965, grad_fn=<DivBackward0>) bce:  tensor(0.0595, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9923, grad_fn=<DivBackward0>) bce:  tensor(-0.0025, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.3051, grad_fn=<DivBackward0>) bce:  tensor(0.0504, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9445, grad_fn=<DivBackward0>) bce:  tensor(-0.0195, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9002, grad_fn=<DivBackward0>) bce:  tensor(-0.0120, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.8418, grad_fn=<DivBackward0>) bce:  tensor(-0.0623, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0203, grad_fn=<DivBackward0>) bce:  tensor(-0.0686, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1109, grad_fn=<DivBackward0>) bce:  tensor(-0.0160, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0348, grad_fn=<DivBackward0>) bce:  tensor(0.1741, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9801, grad_fn=<DivBackward0>) bce:  tensor(-0.1942, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0717, grad_fn=<DivBackward0>) bce:  tensor(-0.2070, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.8905, grad_fn=<DivBackward0>) bce:  tensor(0.1845, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9894, grad_fn=<DivBackward0>) bce:  tensor(-0.0886, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0869, grad_fn=<DivBackward0>) bce:  tensor(-0.0450, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0877, grad_fn=<DivBackward0>) bce:  tensor(0.1146, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0363, grad_fn=<DivBackward0>) bce:  tensor(-0.2263, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1274, grad_fn=<DivBackward0>) bce:  tensor(0.0640, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1355, grad_fn=<DivBackward0>) bce:  tensor(-0.3798, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2568, grad_fn=<DivBackward0>) bce:  tensor(-0.0624, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2904, grad_fn=<DivBackward0>) bce:  tensor(-0.4244, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0075, grad_fn=<DivBackward0>) bce:  tensor(0.0311, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1005, grad_fn=<DivBackward0>) bce:  tensor(0.0768, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0923, grad_fn=<DivBackward0>) bce:  tensor(-0.0739, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1623, grad_fn=<DivBackward0>) bce:  tensor(-0.2581, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2462, grad_fn=<DivBackward0>) bce:  tensor(-0.1274, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9004, grad_fn=<DivBackward0>) bce:  tensor(-0.1141, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0818, grad_fn=<DivBackward0>) bce:  tensor(0.0606, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1416, grad_fn=<DivBackward0>) bce:  tensor(-0.2604, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9937, grad_fn=<DivBackward0>) bce:  tensor(-0.1403, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9342, grad_fn=<DivBackward0>) bce:  tensor(-0.0280, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0598, grad_fn=<DivBackward0>) bce:  tensor(0.0034, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2919, grad_fn=<DivBackward0>) bce:  tensor(-0.1465, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9401, grad_fn=<DivBackward0>) bce:  tensor(-0.1372, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0203, grad_fn=<DivBackward0>) bce:  tensor(0.0037, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1628, grad_fn=<DivBackward0>) bce:  tensor(-0.2711, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9410, grad_fn=<DivBackward0>) bce:  tensor(-0.3230, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9913, grad_fn=<DivBackward0>) bce:  tensor(-0.0711, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1114, grad_fn=<DivBackward0>) bce:  tensor(-0.1310, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.8988, grad_fn=<DivBackward0>) bce:  tensor(-0.2010, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9526, grad_fn=<DivBackward0>) bce:  tensor(-0.2711, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0914, grad_fn=<DivBackward0>) bce:  tensor(-0.1506, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1362, grad_fn=<DivBackward0>) bce:  tensor(-0.1419, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0899, grad_fn=<DivBackward0>) bce:  tensor(-0.3445, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2951, grad_fn=<DivBackward0>) bce:  tensor(-0.0087, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1622, grad_fn=<DivBackward0>) bce:  tensor(-0.0919, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2652, grad_fn=<DivBackward0>) bce:  tensor(-0.3236, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9903, grad_fn=<DivBackward0>) bce:  tensor(-0.0280, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1489, grad_fn=<DivBackward0>) bce:  tensor(-0.1081, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2252, grad_fn=<DivBackward0>) bce:  tensor(-0.0741, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0223, grad_fn=<DivBackward0>) bce:  tensor(-0.1269, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0675, grad_fn=<DivBackward0>) bce:  tensor(0.0643, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9780, grad_fn=<DivBackward0>) bce:  tensor(0.0124, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9278, grad_fn=<DivBackward0>) bce:  tensor(-0.0895, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0628, grad_fn=<DivBackward0>) bce:  tensor(-0.2752, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9415, grad_fn=<DivBackward0>) bce:  tensor(-0.2104, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9695, grad_fn=<DivBackward0>) bce:  tensor(0.0909, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1039, grad_fn=<DivBackward0>) bce:  tensor(-0.0440, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1556, grad_fn=<DivBackward0>) bce:  tensor(0.0032, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9536, grad_fn=<DivBackward0>) bce:  tensor(-0.0050, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0835, grad_fn=<DivBackward0>) bce:  tensor(-0.1890, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9710, grad_fn=<DivBackward0>) bce:  tensor(-0.1178, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2340, grad_fn=<DivBackward0>) bce:  tensor(-0.1506, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9518, grad_fn=<DivBackward0>) bce:  tensor(-0.1417, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1165, grad_fn=<DivBackward0>) bce:  tensor(0.0022, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1511, grad_fn=<DivBackward0>) bce:  tensor(-0.0234, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1226, grad_fn=<DivBackward0>) bce:  tensor(0.2856, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0852, grad_fn=<DivBackward0>) bce:  tensor(-0.0504, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1067, grad_fn=<DivBackward0>) bce:  tensor(-0.0207, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2334, grad_fn=<DivBackward0>) bce:  tensor(0.0411, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2194, grad_fn=<DivBackward0>) bce:  tensor(0.1771, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0972, grad_fn=<DivBackward0>) bce:  tensor(-0.0698, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.8506, grad_fn=<DivBackward0>) bce:  tensor(-0.1915, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0799, grad_fn=<DivBackward0>) bce:  tensor(-0.0570, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1629, grad_fn=<DivBackward0>) bce:  tensor(-0.2231, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0159, grad_fn=<DivBackward0>) bce:  tensor(-0.2063, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.3402, grad_fn=<DivBackward0>) bce:  tensor(-0.2613, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1804, grad_fn=<DivBackward0>) bce:  tensor(0.4314, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9115, grad_fn=<DivBackward0>) bce:  tensor(-0.1451, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1245, grad_fn=<DivBackward0>) bce:  tensor(-0.3368, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9845, grad_fn=<DivBackward0>) bce:  tensor(-0.1618, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.8332, grad_fn=<DivBackward0>) bce:  tensor(0.0329, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2727, grad_fn=<DivBackward0>) bce:  tensor(-0.2030, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1106, grad_fn=<DivBackward0>) bce:  tensor(-0.0380, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0220, grad_fn=<DivBackward0>) bce:  tensor(-0.3539, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.8946, grad_fn=<DivBackward0>) bce:  tensor(-0.1119, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2071, grad_fn=<DivBackward0>) bce:  tensor(0.1273, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1079, grad_fn=<DivBackward0>) bce:  tensor(0.0524, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.8407, grad_fn=<DivBackward0>) bce:  tensor(-0.2725, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0451, grad_fn=<DivBackward0>) bce:  tensor(-0.2017, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2012, grad_fn=<DivBackward0>) bce:  tensor(0.2784, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1368, grad_fn=<DivBackward0>) bce:  tensor(-0.3612, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2205, grad_fn=<DivBackward0>) bce:  tensor(-0.1566, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.3022, grad_fn=<DivBackward0>) bce:  tensor(0.1221, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1770, grad_fn=<DivBackward0>) bce:  tensor(-0.2073, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.8421, grad_fn=<DivBackward0>) bce:  tensor(-0.0225, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9878, grad_fn=<DivBackward0>) bce:  tensor(0.1858, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9805, grad_fn=<DivBackward0>) bce:  tensor(-0.0460, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1173, grad_fn=<DivBackward0>) bce:  tensor(-0.3044, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0187, grad_fn=<DivBackward0>) bce:  tensor(-0.0905, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.2407, grad_fn=<DivBackward0>) bce:  tensor(-0.1573, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "100 1.0833653211593628\n",
      "kl:  tensor(0.9760, grad_fn=<DivBackward0>) bce:  tensor(-0.0277, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9423, grad_fn=<DivBackward0>) bce:  tensor(0.0007, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9579, grad_fn=<DivBackward0>) bce:  tensor(-0.0192, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0230, grad_fn=<DivBackward0>) bce:  tensor(-0.0639, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1006, grad_fn=<DivBackward0>) bce:  tensor(-0.1731, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0139, grad_fn=<DivBackward0>) bce:  tensor(0.0494, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0898, grad_fn=<DivBackward0>) bce:  tensor(-0.2703, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0943, grad_fn=<DivBackward0>) bce:  tensor(-0.3152, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0641, grad_fn=<DivBackward0>) bce:  tensor(-0.0740, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9752, grad_fn=<DivBackward0>) bce:  tensor(-0.0016, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0218, grad_fn=<DivBackward0>) bce:  tensor(0.2034, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9391, grad_fn=<DivBackward0>) bce:  tensor(-0.3374, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0054, grad_fn=<DivBackward0>) bce:  tensor(-0.3842, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.3548, grad_fn=<DivBackward0>) bce:  tensor(-0.1134, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1117, grad_fn=<DivBackward0>) bce:  tensor(-0.2665, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9949, grad_fn=<DivBackward0>) bce:  tensor(0.0838, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1966, grad_fn=<DivBackward0>) bce:  tensor(-0.3274, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0037, grad_fn=<DivBackward0>) bce:  tensor(-0.2954, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1000, grad_fn=<DivBackward0>) bce:  tensor(-0.0579, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1592, grad_fn=<DivBackward0>) bce:  tensor(-0.1198, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.0623, grad_fn=<DivBackward0>) bce:  tensor(-0.3047, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(0.9622, grad_fn=<DivBackward0>) bce:  tensor(-0.2708, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "kl:  tensor(1.1075, grad_fn=<DivBackward0>) bce:  tensor(0.0593, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "for t in range(1000):\n",
    "    # 1. sample a mini-batch of size bb = 32\n",
    "    x, y = train_dataloader.__next__()\n",
    "\n",
    "    x = torch.Tensor(x)\n",
    "    y = torch.Tensor(y)\n",
    "\n",
    "    encoder_optimzation.zero_grad()\n",
    "    decoder_optimzation.zero_grad()\n",
    "\n",
    "    encoding = encoder(x)\n",
    "    mu       = encoding[:, :8]\n",
    "    sigma    = encoding[:, 8:]\n",
    "    decoding = decoder(mu)\n",
    "\n",
    "    total_loss = loss(x.reshape((x.shape[0], -1)), mu, sigma, decoding)\n",
    "    total_loss.backward()\n",
    "\n",
    "    encoder_optimzation.step()\n",
    "    decoder_optimzation.step()\n",
    "\n",
    "    train_loss_list.append(total_loss.item())\n",
    "    \n",
    "    if t % 100 == 0:\n",
    "        print(t, total_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "w_bmE5CdSB0c",
    "outputId": "4d40b500-cdb1-4e7d-b6bc-f4d475fe2c45"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(2, 4)\n",
    "\n",
    "num_samples = 4\n",
    "index = np.random.randint(len(resized_val_x), size=8)\n",
    "i = 0\n",
    "\n",
    "# for r in range(2):\n",
    "for c in range(num_samples):\n",
    "    x = resized_val_x[index[i]]\n",
    "    axs[0, c].imshow(x.reshape(14, 14), cmap=plt.cm.gray)\n",
    "    axs[0, c].set_title(str(valY[index[i]]))\n",
    "    axs[0, c].axis('off')\n",
    "    \n",
    "    x = torch.Tensor(x).unsqueeze(0)\n",
    "    encoding = encoder(x)\n",
    "    mu = encoding[:, :8]\n",
    "    sigma = encoding[:, 8:]\n",
    "    decoding = decoder(mu).reshape((14, 14)).detach().numpy()\n",
    "\n",
    "    axs[1, c].imshow(decoding, cmap=plt.cm.gray)\n",
    "    axs[1, c].set_title(str(valY[index[i]]))\n",
    "    axs[1, c].axis('off')\n",
    "    i+=1\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for t in range(1000):\n",
    "    x, y = train_dataloader.__next__()\n",
    "    x = torch.Tensor(x)\n",
    "    y = torch.Tensor(y).long()\n",
    "    \n",
    "    enc_optim.zero_grad()\n",
    "    dec_optim.zero_grad()\n",
    "    \n",
    "    encoding = enc(x)\n",
    "    mu    = encoding[:,  :8]\n",
    "    sigma = encoding[:, 8: ]\n",
    "    decoding = dec(x)\n",
    "    \n",
    "    kl = loss(x, mu, sigma, decoding)\n",
    "    kl.backward()\n",
    "\n",
    "    enc_optim.step()\n",
    "    dec_optim.step()\n",
    "    \n",
    "    print(t, total_loss.item())\n",
    "    train_loss_list.append(total_loss.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
