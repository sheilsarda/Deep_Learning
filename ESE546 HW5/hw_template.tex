
\documentclass[11pt, reqno, letterpaper, twoside]{amsart}

\linespread{1.2}



\usepackage{amssymb, bm, mathtools}
\usepackage[pdftex, xetex]{graphicx}
\usepackage{enumerate, setspace}
\usepackage{float, colortbl, tabularx, longtable, multirow, subcaption, environ, wrapfig, textcomp, booktabs}
\usepackage{pgf, tikz, framed}
\usepackage[normalem]{ulem}
\usetikzlibrary{arrows,positioning,automata,shadows,fit,shapes}
\usepackage[english]{babel}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

 
\usepackage{microtype}
\microtypecontext{spacing=nonfrench}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem{solution}[theorem]{Solution}

\usepackage{times}
\title{ESE 546, Fall 2020\\[0.1in]
Homework 5}
\author{
Sheil Sarda [sheils@seas]
}

\begin{document}
\maketitle
\begin{solution}[Time spent: 2 hour]
\begin{enumerate}
\item To Prove:
\begin{align*}
\mathrm{KL}(q \| p)=\beta \sum_{w \in \mathcal{W}} q(w) \Phi(w)+\sum_{w \in \mathcal{W}} q(w) \log q(w)+\log Z(\beta)
\end{align*}

Starting with the definition of KL Divergence:
\begin{align*}
\mathrm{KL}(q \| p) &=\sum_{w \in \mathcal{W}} q(w) \frac{q(w)}{p(w)} \\
				    &=\sum_{w \in \mathcal{W}} q(w) \frac{Z q(w)}{e^{-\beta \Phi(w)}} &\text{Since $p(w) = \frac{e^{\beta \Phi(w)}}{Z} $} \\
				    &=\sum_{w \in \mathcal{W}} q(w) \left[log( Z ) + log( q(w) ) + \beta \Phi(w)\right] \\
				    &= log( Z ) \sum_{w \in \mathcal{W}} q(w) + \sum_{w \in \mathcal{W}} q(w)  log( q(w) ) +   \beta \sum_{w \in \mathcal{W}} q(w) \Phi(w)	\\			    
				    &= log( Z ) + \sum_{w \in \mathcal{W}} q(w)  log( q(w) ) +   \beta \sum_{w \in \mathcal{W}} q(w) \Phi(w)	 &\text{Since $\sum_{w \in \mathcal{W}} q(w) = 1 $} \\
\end{align*}
\qed
\newpage
\item To Prove:
\begin{align*}
\underset{w \sim p(w)}{\mathbb{E}}[\Phi(w)]=-\frac{\partial \log Z(\beta)}{\partial \beta} 
\end{align*}
Starting from the LHS:
\begin{align*}
\underset{w \sim p(w)}{\mathbb{E}}[\Phi(w)] &=\sum_{w \in \mathcal{W}} p(w) \Phi(w)
\end{align*}
Substituting for $\Phi(w)$:
\begin{align*}
1  &= \sum_{w \in \mathcal{W}} p(w) = \sum_{w \in \mathcal{W}}\frac{e^{\beta \Phi(w)}}{Z} \\
\implies Z &= \sum_{w \in \mathcal{W}}e^{\beta \Phi(w)} \\
\implies \frac{\partial Z}{\partial \beta} &= \sum_{w \in \mathcal{W}}e^{\beta \Phi(w)} \left( -\Phi(w) \right)  \\
&\text{By definition of $p(w)$, we also know: } \\
&p(w) = \frac{e^{\beta \Phi(w)}}{Z}  \implies Z \times p(w) = e^{\beta \Phi(w)} \\
&\text{Substituting into the RHS: } \\
\frac{\partial Z}{\partial \beta} &= \sum_{w \in \mathcal{W}} -Z \times p(w) \times \Phi(w) \\
-\frac{\partial log Z}{\partial \beta} &= \sum_{w \in \mathcal{W}} p(w) \times \Phi(w) \\
-\frac{\partial log Z}{\partial \beta} &= \underset{w \sim p(w)}{\mathbb{E}}[\Phi(w)] \\
\end{align*}
\qed
\end{enumerate}
\end{solution}
\newpage

\begin{solution}[Time spent: 10 hours]
Plots from the Jupyter notebook attached here for reference.

\begin{enumerate}
\item Plot of Binarized and 14*14 subsampled images of MNIST
    \begin{figure}[H]
    	\centering
    	\includegraphics[width=1\textwidth]{images/subsample}
    	\label{fig:my_label}
    \end{figure}
\item 
Encoder and Decoder classes.
\begin{verbatim}
class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        self.fc1 = nn.Linear(196, 128)
        self.fc2 = nn.Linear(128, 16)
        self.fc3 = nn.Linear(128, 16)

    def forward(self, x):
        reshaped = x.reshape(x.shape[0], -1)
        out = torch.tanh(self.fc1(reshaped))
        fc2_out = self.fc2(out)
        fc3_out = self.fc3(out)
        
        mu = (fc2_out[:, :8] + fc3_out[:, :8])/2
        logvar = (fc2_out[:, 8:] + fc3_out[:, 8:])/2
        
        std = logvar.mul(0.5).exp_()
        eps = torch.randn_like(std)
        z = eps.mul(std).add_(mu)  
        return z, mu, logvar
        
class Decoder(nn.Module):
    def __init__(self):
        super(Decoder, self).__init__()
        self.fc1 = nn.Linear(8, 128)
        self.fc2 = nn.Linear(128, 196)

    def forward(self, x):
        out = torch.tanh(self.fc1(x))
        out = torch.sigmoid(self.fc2(out))
        return out
\end{verbatim}

Loss functions.

\begin{verbatim}
def KL(mu, sigma):
    contribution = 1 + sigma - mu**2 - torch.exp(sigma)
    return (torch.sum(-contribution/2))
    
def TotalLoss(x, mu, sigma, decoding):
    kl_loss = KL(mu, sigma)
    
    bce =  F.binary_cross_entropy(decoding, 
                                  x.view(-1, 196), 
                                  reduction='sum')     
    return (kl_loss + bce, kl_loss, bce)
\end{verbatim}
\newpage
\item Plot of first and second term of ELBO as a function of the number of weight updates.
\begin{figure}[H]
    	\centering
    	\makebox[\textwidth][c]{\includegraphics[width=1.2\textwidth]{images/elbo}}%
    	\label{fig:my_label}
    \end{figure}

\item Reconstruction of MNIST images using the Autoencoder.
\begin{figure}[H]
    	\centering
    	\makebox[\textwidth][c]{\includegraphics[width=0.7\textwidth]{images/reconstruction_1}}%
    	\label{fig:my_label}
    \end{figure}
\begin{figure}[H]
    	\centering
    	\makebox[\textwidth][c]{\includegraphics[width=0.7\textwidth]{images/reconstruction_2}}%
    	\label{fig:my_label}
    \end{figure}
\item Images created by sampling from the generative model and running the decoder.
\begin{figure}[H]
    	\centering
    	\makebox[\textwidth][c]{\includegraphics[width=0.7\textwidth]{images/random}}%
    	\label{fig:my_label}
    \end{figure}


\end{enumerate}

\end{solution}



\end{document}