\documentclass[11pt, reqno, letterpaper, twoside]{amsart}
\linespread{1.2}
\usepackage[margin=1.25in]{geometry}

\usepackage{amssymb, bm, mathtools}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[pdftex, xetex]{graphicx}
\usepackage{enumerate, setspace}
\usepackage{float, colortbl, tabularx, longtable, multirow, subcaption, environ, wrapfig, textcomp, booktabs}
\usepackage{pgf, tikz, framed}
\usepackage[normalem]{ulem}
\usetikzlibrary{arrows,positioning,automata,shadows,fit,shapes}
\usepackage[english]{babel}

\usepackage{microtype}
\microtypecontext{spacing=nonfrench}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem{solution}[theorem]{Solution}

\usepackage{times}
\title{ESE 546, Fall 2020\\[0.1in]
Homework 3}
\author{
Sheil Sarda [sheils@seas],\\
Collaborators: Rahul M. [rmag@seas]
}

\begin{document}
\maketitle

\begin{solution}[Time spent: 5 hours]

\begin{enumerate}
	\item Prove that co-coercivity implies Lipschitz continuity.
\begin{align*}
\langle\nabla f(x)-\nabla f(y), x-y\rangle \geq \frac{1}{L}\|\nabla f(x)-\nabla f(y)\|^{2} \Rightarrow\|\nabla f(x)-\nabla f(y)\| \leq L\|x-y\|
\end{align*}

According to the Cauchy-Shwarz inequality,   $$ \| \langle u, v \rangle \| \leq \|u\| \|v\|$$

Applying Cauchy Schwarz to the RHS of the given inequality and multiplying by $L$ on both sides:
\begin{align*}
 L \langle\nabla f(x)-\nabla f(y), x-y\rangle &\leq \|\nabla f(x)-\nabla f(y) \| \|x-y\| L \\
\|\nabla f(x)-\nabla f(y)\|^{2} \leq  L \langle\nabla f(x)-\nabla f(y), x-y\rangle &\leq \|\nabla f(x)-\nabla f(y) \| \|x-y\| L  
\end{align*}

Eliminating the middle term in the inequality above:
\begin{align*}
 \|\nabla f(x)-\nabla f(y)\|^{2} & \leq \|\nabla f(x)-\nabla f(y) \| \|x-y\| L \\
\implies  \|\nabla f(x)-\nabla f(y)\| & \leq L \|x-y\| 
\end{align*}
\qed

\item Prove that the Lipschitz continuity implies co-coercivity.
Consider 2 functions:
\begin{align*}
g(z) &=f(z)-\langle\nabla f(x), z\rangle \\
h(z) &=f(z)-\langle\nabla f(y), z\rangle
\end{align*}

Applying the descent lemma to $g(y)$,
\begin{align*}
\frac{1}{2 L}\|\nabla g(y)\| &\leq g(y)-g(x) \\
\Rightarrow \frac{1}{2 L}\|\nabla g(y)\| &\leq f(y)-f(x)-\langle\nabla f(x), y\rangle - \langle\nabla f(x), x\rangle\\
\Rightarrow \frac{1}{2 L}\|\nabla g(y)\| &\leq f(y)-f(x)-\langle\nabla f(x), y-x\rangle 
\end{align*}

Therefore,
\begin{align*}
f(y)-f(x)-\langle\nabla f(x), y-x\rangle \geq \frac{1}{2 L}\|\nabla f(y)-\nabla f(x)\| \\
f(x)-f(y)-\langle\nabla f(y), x-y\rangle \geq \frac{1}{2 L}\|\nabla f(x)-\nabla f(y)\| 
\end{align*} 
Adding the two above inequalities, we have
$$ \langle\nabla f(x)-f(y), x-y\rangle \geq \frac{1}{L}\|\nabla f(x)-\nabla f(y)\| $$

\item Prove that $m \leq \| \nabla^2 f(x) \|_{2} \leq L$
Applying the mean value theorem to $\nabla f(x)$,
\begin{align*}
\nabla^2 f(x) = \frac{\nabla f(b) - \nabla f(a)}{b - a}
\end{align*}
Applying the result from (1) of this question $\implies \nabla^2 f(x) \leq L$.

For any strongly convex function $\ell$, the following must hold
\begin{align*}
\ell(w)+\left\langle\nabla \ell(w), w^{\prime}-w\right\rangle+\frac{m}{2}\left\|w^{\prime}-w\right\|^{2} \leq \ell\left(w^{\prime}\right) 
\end{align*} 

Since we know that $f(x)$ is strongly convex, $\nabla^{2} f(x) \succeq m I_{p \times p} = m$ (from Lecture Notes 09). Combining both results, we get:
\begin{align*}
m \leq \| \nabla^2 f(x) \|_{2} \leq L
\end{align*}
\qed

\end{enumerate}

\end{solution}

\clearpage
\begin{solution}[Time spent: 3 hours]
To prove:
\begin{align*}
	\min _{w} \underset{R}{\mathbb{E}}\left[\|y-(R \odot X) w\|_{2}^{2}\right] &=\min _{\tilde{w}}\|y-X \tilde{w}\|_{2}^{2}+\left(\frac{p}{1-p}\right) \tilde{w}^{\top} \operatorname{diag}\left(X^{\top} X\right) \tilde{w} \\
	\text { where } \tilde{w} &=(1-p) w
\end{align*}
For context (from Lecture Notes 07), \begin{itemize}
	\item Each row of matrix $R$ consists of the dropout mask for the $i^{th}$ row $x^i$ of the data matrix $X$.
	\item Each entry of $R$ is a Bernoulli random variable with probability $1-p$ of being 1.
	\item For linear regression, dropout is equivalent to weight decay where the coefficient $\alpha$ depends on the diagonal of the data covariance and is different for different weights.
	\item If a particular data dimension varies a lot $\implies X^T X$ is large, then dropout tries to squeeze its weight to zero.
	\item If $p = 0$, most activations are retained by the mask and regularization is small.
	\item Given weights $w$ of a model trained using dropout, we can compute the committee average over models created using dropout masks simply by scaling the weights by a factor $1 - p \implies \tilde{w} = (1-p)w$ is the effective weight.
\end{itemize}
The RHS can be re-written as:
\begin{align*}
&	\min _{w} \underset{R}{\mathbb{E}}\left[\|y-(R \odot X) w\|_{2}^{2}\right] \\
\implies &	\min _{w} \left[\|y-(X (1-p)) w\|_{2}^{2}\right] &\text{since $R$ comprises of Bernoulli variables}\\
\implies &	\min _{w} \left[\|y-X \tilde{w} \|_{2}^{2}\right] &\text{by definition of $\tilde{w}$}\\
\end{align*}
Now, to go from $$\min _{w} \left[\|y-X \tilde{w} \|_{2}^{2}\right] \rightarrow \min _{\tilde{w}}\|y-X \tilde{w}\|_{2}^{2}+\left(\frac{p}{1-p}\right) \tilde{w}^{\top} \operatorname{diag}\left(X^{\top} X\right) \tilde{w} $$

We observe that $\left(\frac{p}{1-p}\right) \tilde{w}^{\top} \operatorname{diag}\left(X^{\top} X\right) \tilde{w} $ can be re-written as
\begin{align*}
& \left(\frac{p}{1-p}\right) \tilde{w}^{\top} \operatorname{diag}\left(X^{\top} X\right) \tilde{w} \\
\implies & \left(\frac{p}{1-p}\right) \tilde{w}^{\top} \operatorname{diag}\left(X^{\top} X\right) w (1-p) \\
\implies & p(1-p) \; w^{\top} \operatorname{diag}\left(X^{\top} X\right) w \\
\end{align*}
\end{solution}

\clearpage
\begin{solution}[Time spent: 3 hours]
Population risk of a regression is
\begin{align*}
R(f)=\int|f(x)-y|^{2} P(x, y) \mathrm{d} x \mathrm{d} y
\end{align*}

Prove that model that minimizes the population risk is
\begin{align*}
f^{*}=\underset{f}{\operatorname{argmin}} R(f)=\mathbb{E}[y \mid x]
\end{align*}


Notes:
This is the MSE loss. Minimum of that is the conditional mean of $y | x$. If just trying to optimize $R(f)$, how do you take into account the integral?

The conditional expectation is essentially the integral of $y \times p(y | x) \times dy$. Just looking for the minimum, so write some equations to find the minimum (differentiation with respect to $f$).

You are finding the min of the regressor by differentiating.

Proof:

\begin{align*}
\| f(x) = 
\end{align*}

\end{solution}

\end{document}7