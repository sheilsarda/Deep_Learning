\documentclass[11pt, reqno, letterpaper, twoside]{amsart}
\linespread{1.2}
\usepackage[margin=1.25in]{geometry}

\usepackage{amssymb, bm, mathtools}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[pdftex, xetex]{graphicx}
\usepackage{enumerate, setspace}
\usepackage{float, colortbl, tabularx, longtable, multirow, subcaption, environ, wrapfig, textcomp, booktabs}
\usepackage{pgf, tikz, framed}
\usepackage[normalem]{ulem}
\usetikzlibrary{arrows,positioning,automata,shadows,fit,shapes}
\usepackage[english]{babel}

\usepackage{microtype}
\microtypecontext{spacing=nonfrench}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem{solution}[theorem]{Solution}

\usepackage{times}
\title{ESE 546, Fall 2020\\[0.1in]
Homework 3}
\author{
Sheil Sarda [sheils@seas],\\
Collaborators: Rahul M. [rmag@seas]
}

\begin{document}
\maketitle

\begin{solution}[Time spent: 5 hours]

\begin{enumerate}
	\item Prove that co-coercivity implies Lipschitz continuity.
\begin{align*}
\langle\nabla f(x)-\nabla f(y), x-y\rangle \geq \frac{1}{L}\|\nabla f(x)-\nabla f(y)\|^{2} \Rightarrow\|\nabla f(x)-\nabla f(y)\| \leq L\|x-y\|
\end{align*}

According to the Cauchy-Shwarz inequality,   $$ \| \langle u, v \rangle \| \leq \|u\| \|v\|$$

Applying Cauchy Schwarz to the RHS of the given inequality and multiplying by $L$ on both sides:
\begin{align*}
 L \langle\nabla f(x)-\nabla f(y), x-y\rangle &\leq \|\nabla f(x)-\nabla f(y) \| \|x-y\| L \\
\|\nabla f(x)-\nabla f(y)\|^{2} \leq  L \langle\nabla f(x)-\nabla f(y), x-y\rangle &\leq \|\nabla f(x)-\nabla f(y) \| \|x-y\| L  
\end{align*}

Eliminating the middle term in the inequality above:
\begin{align*}
 \|\nabla f(x)-\nabla f(y)\|^{2} & \leq \|\nabla f(x)-\nabla f(y) \| \|x-y\| L \\
\implies  \|\nabla f(x)-\nabla f(y)\| & \leq L \|x-y\| 
\end{align*}
\qed

\item Prove that the Lipschitz continuity implies co-coercivity.
Consider 2 functions:
\begin{align*}
g(z) &=f(z)-\langle\nabla f(x), z\rangle \\
h(z) &=f(z)-\langle\nabla f(y), z\rangle
\end{align*}

Applying the descent lemma to $g(y)$,
\begin{align*}
\frac{1}{2 L}\|\nabla g(y)\| &\leq g(y)-g(x) \\
\Rightarrow \frac{1}{2 L}\|\nabla g(y)\| &\leq f(y)-f(x)-\langle\nabla f(x), y\rangle - \langle\nabla f(x), x\rangle\\
\Rightarrow \frac{1}{2 L}\|\nabla g(y)\| &\leq f(y)-f(x)-\langle\nabla f(x), y-x\rangle 
\end{align*}

Therefore,
\begin{align*}
f(y)-f(x)-\langle\nabla f(x), y-x\rangle \geq \frac{1}{2 L}\|\nabla f(y)-\nabla f(x)\| \\
f(x)-f(y)-\langle\nabla f(y), x-y\rangle \geq \frac{1}{2 L}\|\nabla f(x)-\nabla f(y)\| 
\end{align*} 
Adding the two above inequalities, we have
$$ \langle\nabla f(x)-f(y), x-y\rangle \geq \frac{1}{L}\|\nabla f(x)-\nabla f(y)\| $$

\item Prove that $m \leq \| \nabla^2 f(x) \|_{2} \leq L$
Applying the mean value theorem to $\nabla f(x)$,
\begin{align*}
\nabla^2 f(x) = \frac{\nabla f(b) - \nabla f(a)}{b - a}
\end{align*}
Applying the result from (1) of this question $\implies \nabla^2 f(x) \leq L$.

For any strongly convex function $\ell$, the following must hold
\begin{align*}
\ell(w)+\left\langle\nabla \ell(w), w^{\prime}-w\right\rangle+\frac{m}{2}\left\|w^{\prime}-w\right\|^{2} \leq \ell\left(w^{\prime}\right) 
\end{align*} 

Since we know that $f(x)$ is strongly convex, $\nabla^{2} f(x) \succeq m I_{p \times p} = m$ (from Lecture Notes 09). Combining both results, we get:
\begin{align*}
m \leq \| \nabla^2 f(x) \|_{2} \leq L
\end{align*}
\qed

\end{enumerate}

\end{solution}

\clearpage
\begin{solution}[Time spent: 6 hours]
To prove:
\begin{align*}
	\min _{w} \underset{R}{\mathbb{E}}\left[\|y-(R \odot X) w\|_{2}^{2}\right] &=\min _{\tilde{w}}\|y-X \tilde{w}\|_{2}^{2}+\left(\frac{p}{1-p}\right) \tilde{w}^{\top} \operatorname{diag}\left(X^{\top} X\right) \tilde{w} \\
	\text { where } \tilde{w} &=(1-p) w
\end{align*}
For context (from Lecture Notes 07), \begin{itemize}
	\item Each row of matrix $R$ consists of the dropout mask for the $i^{th}$ row $x^i$ of the data matrix $X$.
	\item Each entry of $R$ is a Bernoulli random variable with probability $1-p$ of being 1.
	\item For linear regression, dropout is equivalent to weight decay where the coefficient $\alpha$ depends on the diagonal of the data covariance and is different for different weights.
	\item If a particular data dimension varies a lot $\implies X^T X$ is large, then dropout tries to squeeze its weight to zero.
	\item If $p = 0$, most activations are retained by the mask and regularization is small.
	\item Given weights $w$ of a model trained using dropout, we can compute the committee average over models created using dropout masks simply by scaling the weights by a factor $1 - p \implies \tilde{w} = (1-p)w$ is the effective weight.
\end{itemize}
First, we determine $\mathbb{E}\left[R\right]$. Each element of $R$ is a Bernoulli random variable. Therefore,
\begin{itemize}
\item Case 1: $R_{ij} = 1$. This occurs with probability $1-p$.
\item Case 2: $R_{ij} = 0$. This occurs with probability $p$.
\end{itemize}
Thus, $\mathbb{E}[R \odot X] = X(1-p)$

We simplify the RHS to eliminate the L2 norm,
\begin{align*}
&	\min _{w} \underset{R}{\mathbb{E}}\left[\|y-(R \odot X) w\|_{2}^{2}\right] \\
\implies &	\min _{w} \underset{R}{\mathbb{E}}\left[y^2+w^{\top}(R \odot X)^{\top}(R \odot X) w - 2y(R \odot X)w \right] \\
\implies &	\min _{w} y^2+ \underset{R}{\mathbb{E}}\left[w^{\top}(R \odot X)^{\top}(R \odot X) w \right] - \underset{R}{\mathbb{E}}\left[2y(R \odot X)w \right] &\text{Applying LOE} \\
\implies &	\min _{w} y^2+ \underset{R}{\mathbb{E}}\left[w^{\top}(R \odot X)^{\top}(R \odot X) w \right] - 2yX(1-p)w  \\
\implies &	\min _{w} y^2+ \underset{R}{\mathbb{E}}\left[w^{\top}(R \odot X)^{\top}(R \odot X) w \right] - 2yX\tilde{w}  \\
\implies &	\min _{w} y^2 - 2yX\tilde{w} + \tilde{w}^{\top}X^{\top}X\tilde{w} - \tilde{w}^{\top}X^{\top}X\tilde{w} + \underset{R}{\mathbb{E}}\left[w^{\top}(R \odot X)^{\top}(R \odot X) w \right]  &\text{Completing the square} \\
\implies &	\min _{w} \| y - X\tilde{w} \|_2^2  - \tilde{w}^{\top}X^{\top}X\tilde{w} + \underset{R}{\mathbb{E}}\left[w^{\top}(R \odot X)^{\top}(R \odot X) w \right]  
\end{align*}
The expression $ \underset{R}{\mathbb{E}}\left[(R \odot X)^{\top}(R \odot X) \right]  $ can be simplified to two cases. \begin{itemize}
\item Case 1: Elements on the main diagonal of $ \underset{R}{\mathbb{E}}\left[(R_ \odot X)^{\top}(R \odot X) \right]  = (1-p) * X^{\top}X$
\item Case 2: Elements off the main diagonal $ \underset{R}{\mathbb{E}}\left[(R_ \odot X)^{\top}(R \odot X) \right]  = (1-p)^2 * X^{\top}X$
\end{itemize}
We can express this product in inline notation as
\begin{align*}
\underset{R}{\mathbb{E}}\left[(R \odot X)^{\top}(R \odot X) \right]  & =   \operatorname{diag}\left(X^{\top} X\right) (1-p) +  \left(\left(X^{\top} X\right) - \operatorname{diag}\left(X^{\top} X\right)\right) (1-p)^2 \\
& =   \operatorname{diag}\left(X^{\top} X\right) (1-p) +  \left(X^{\top} X\right)(1-p)^2 - \operatorname{diag}\left(X^{\top} X\right) (1-p)^2 \\
& =   \operatorname{diag}\left(X^{\top} X\right) (1-p)p +  \left(X^{\top} X\right)(1-p)^2\\
\end{align*}

Substituting this expression into the above equation,
\begin{align*}
&	\min _{w} \| y - X\tilde{w} \|_2^2  - \tilde{w}^{\top}X^{\top}X\tilde{w} +\\
&\qquad w^{\top}\left( \operatorname{diag}\left(X^{\top} X\right) (1-p)p +  \left(X^{\top} X\right)(1-p)^2 \right) w \\
\implies &	\min _{w} \| y - X\tilde{w} \|_2^2  - \tilde{w}^{\top}X^{\top}X\tilde{w} + w^{\top} \operatorname{diag}\left(X^{\top} X\right) (1-p)p \; w + \\
&\qquad w^{\top} \left(X^{\top} X\right) (1-p)^2 w \\
\implies &	\min _{w} \| y - X\tilde{w} \|_2^2  - \tilde{w}^{\top}X^{\top}X\tilde{w} + w^{\top} \operatorname{diag}\left(X^{\top} X\right) (1-p)p \; w + \\
&\qquad \tilde{w}^{\top} \left(X^{\top} X\right) \tilde{w} \\
\implies & \min _{\tilde{w}}\|y-X \tilde{w}\|_{2}^{2}+\left(\frac{p}{1-p}\right) \tilde{w}^{\top} \operatorname{diag}\left(X^{\top} X\right) \tilde{w} \qquad \qquad \text{ where } \tilde{w} =(1-p) w \\
\end{align*}
\qed
\end{solution}

\clearpage
\begin{solution}[Time spent: 6 hours]
Population risk of a regression is
\begin{align*}
R(f)=\int|f(x)-y|^{2} P(x, y) \mathrm{d} x \mathrm{d} y
\end{align*}

Prove that model that minimizes the population risk is
\begin{align*}
f^{*}=\underset{f}{\operatorname{argmin}} R(f)=\mathbb{E}[y \mid x]
\end{align*}
First simplify the expression inside the integral as
\begin{align*}
 f(x) -  y  &=   f(x) + \mathbb{E}[y \mid x] - \mathbb{E}[y \mid x] -  y  \\
&=   \left(y - \mathbb{E}[y \mid x] \right)^2 + 2 \left(y - \mathbb{E}[y \mid x] \right) \left( \mathbb{E}[y \mid x] - f(x)\right) + \left( \mathbb{E}[y \mid x] - f(x)\right)^2  \\
\end{align*}
Substituting the above into the given integral
\begin{align*}
R(f)&=\int|f(x)-y|^{2} P(x, y) \mathrm{d} x \mathrm{d} y \\
&=\int|\left(y - \mathbb{E}[y \mid x] \right)^2 + 2 \left(y - \mathbb{E}[y \mid x] \right) \left( \mathbb{E}[y \mid x] - f(x)\right) + \left( \mathbb{E}[y \mid x] - f(x)\right)^2|^{2} P(x, y) \mathrm{d} x \mathrm{d} y \\
&=\int\left(y - \mathbb{E}[y \mid x] \right)^2P(x, y) \mathrm{d} x \mathrm{d} y + 2 \int \left(y - \mathbb{E}[y \mid x] \right) \left( \mathbb{E}[y \mid x] - f(x)\right)P(x, y) \mathrm{d} x \mathrm{d} y + \\
& \qquad \int\left( \mathbb{E}[y \mid x] - f(x)\right)^2 P(x, y) \mathrm{d} x \mathrm{d} y \\
\end{align*}
Since we need to minimize $f$, ignore the first integral term above as it does not contain $f$. Next, we simplify the middle term of the integral as follows
\begin{align*}
&\left(y - \mathbb{E}[y \mid x] \right) \left( \mathbb{E}[y \mid x] - f(x)\right)\\
\implies&\left(\mathbb{E} [y - \mathbb{E}[y \mid x] \mid x]\right) \left( \mathbb{E}[y \mid x] - f(x) \right) &\text{Tower property of conditional expectation}\\
\implies&\left(\mathbb{E} [y \mid x] - \mathbb{E}[\mathbb{E}[y \mid x] \mid x]\right) \left( \mathbb{E}[y \mid x] - f(x) \right) &\text{Applying LOE}\\
\implies&\left(\mathbb{E} [y \mid x] - \mathbb{E}[y \mid x] \right) \left( \mathbb{E}[y \mid x] - f(x) \right) \\
\implies& 0 \\
\end{align*}
Substituting the above result into our original integral, we now need to optimize
\begin{align*}
&f^{*}=\underset{f}{\operatorname{argmin}} \int\left( \mathbb{E}[y \mid x] - f(x)\right)^2 P(x, y) \mathrm{d} x \mathrm{d} y \\
\end{align*}
Analytically, the above integral will be minimized when $f(x) = \mathbb{E}[y \mid x]$. Thus, the optimal classifier $f^{*}(x) = \mathbb{E}[y \mid x]$.
\qed
\end{solution}
\clearpage

\begin{solution}[Time spent: 8 hours]
The RNN class built for this question
\begin{verbatim}
class RNN(nn.Module):

  def __init__(self, input_dim, hidden_dim, output_dim, 
               no_layers = 1):
    super(RNN, self).__init__()
    self.input_dim = input_dim
    self.hidden_dim = hidden_dim
    self.output_dim = output_dim
    self.no_layers = no_layers

    self.rnn_layer = nn.RNN(self.input_dim, 
                            self.hidden_dim, self.no_layers,
                            batch_first = True,
                            nonlinearity='tanh')
    self.linear_out = nn.Linear(self.hidden_dim, self.output_dim)
    self.softmax = nn.LogSoftmax(dim = 1)

  def forward(self, x):
    batch_size = x.size(0)
    hidden = torch.zeros(self.no_layers, 
                         batch_size, 
                         self.hidden_dim).requires_grad_()
    out, hidden = self.rnn_layer(x, hidden.detach())
    out = out.view(batch_size, len(s))
    return out, hidden
\end{verbatim}
Training losses for 100 epochs with 1 Million characters in the training set (40K minibatches), with losses plotted every 1000 minibatches.

\begin{figure}[H]
\includegraphics[scale=0.8]{training losses}
\end{figure}

Training losses at each epoch.
\begin{figure}[H]
	\includegraphics[scale=0.8]{epoch losses}
\end{figure}

\end{solution}

\end{document}