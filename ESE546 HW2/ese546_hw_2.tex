\documentclass[11pt, reqno, letterpaper, twoside]{amsart}
\linespread{1.2}
\usepackage[margin=1.25in]{geometry}

\usepackage{amssymb, bm, mathtools}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[pdftex, xetex]{graphicx}
\usepackage{enumerate, setspace}
\usepackage{float, colortbl, tabularx, longtable, multirow, subcaption, environ, wrapfig, textcomp, booktabs}
\usepackage{pgf, tikz, framed}
\usepackage[normalem]{ulem}
\usetikzlibrary{arrows,positioning,automata,shadows,fit,shapes}
\usepackage[english]{babel}

\usepackage{microtype}
\microtypecontext{spacing=nonfrench}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem{solution}[theorem]{Solution}

\usepackage{times}
\title{ESE 546, Fall 2020\\[0.1in]
Homework 2}
\author{
Sheil Sarda [sheils@seas],\\
Collaborators: Pranav P. [pillaip@seas], Rahul M. [rmag@seas]
}

\begin{document}
\maketitle

\begin{solution}[Time spent: 2 hours]
\begin{enumerate}
\item Peculiarities in the code:
\newline
Batch normalization is applied before \verb|ReLU| in the \verb|resent-18| model. This normalizes the input to each layer for each batch, thus speeding up training.


\item What does \verb|model.train()| do
\newline
It specifies to all the layers that the model is in training mode, so they should behave accordingly; i.e. that gradients are computed correctly.
\newline
What does \verb|model.eval()| do
\newline
It specifies to the layers that it is now in evaluation model, and training has completed; i.e. weights should not be updated further.

The above functionality applies to layers such as dropout layers which have dual functionality.
\newline
Why did we not have it in homework 1
\newline
In the previous homework, we did not have layers which behave differently under training and evaluation conditions.

\item \verb|resnet-18| architecture
\begin{figure}[H]
\includegraphics[scale=0.12]{images/resnet18}
\centering
\caption{Sketch of the resnet-18 architecture}
\end{figure}

Number of parameters in each layer
\begin{table}[H]
	\begin{tabular}{|r|r|}
		\hline
		torch.nn.modules.conv.Conv2d'           & 9408    \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 64      \\ \hline
		torch.nn.modules.conv.Conv2d'           & 36864   \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 64      \\ \hline
		torch.nn.modules.conv.Conv2d'           & 36864   \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 64      \\ \hline
		torch.nn.modules.conv.Conv2d'           & 36864   \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 64      \\ \hline
		torch.nn.modules.conv.Conv2d'           & 36864   \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 64      \\ \hline
		torch.nn.modules.conv.Conv2d'           & 73728   \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 128     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 147456  \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 128     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 8192    \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 128     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 147456  \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 128     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 147456  \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 128     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 294912  \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 256     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 589824  \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 256     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 32768   \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 256     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 589824  \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 256     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 589824  \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 256     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 1179648 \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 512     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 2359296 \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 512     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 131072  \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 512     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 2359296 \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 512     \\ \hline
		torch.nn.modules.conv.Conv2d'           & 2359296 \\ \hline
		torch.nn.modules.batchnorm.BatchNorm2d' & 512     \\ \hline
		torch.nn.modules.linear.Linear'         & 512000  \\ \hline
	\end{tabular}
\end{table}
\item Weight decay should not be applied to the biases of the different layers in the network since its main function is to prevent overfitting by reducing the number of features which are significantly weighted. By restricting the value of the bias term, which is a simple offset applied to each sample, the weights now have to adjust for the bias implicitly which can lead to more features receiving higher weights.
\item \begin{verbatim}
import torchvision.models as models
import numpy as np
resnet18 = models.resnet18()

layers = list(resnet18.modules())
layer_type = {"BatchNorm": 0, "Bias": 0, "Misc": 0}
for layer in layers:
  if hasattr(layer, 'weight'):
    params = 1
    for weight in layer.weight.shape:
      params *= weight
    if "bias=True" in str(layer):
      layer_type["Bias"] += 1000
    if "BatchNorm" in str(layer):
      layer_type["BatchNorm"] += params
    else:
      layer_type["Misc"] += params
\end{verbatim}

\end{enumerate}
\end{solution}

\clearpage
\begin{solution}[Time spent: 4 hours]
\begin{enumerate}
	\item The above problem is not convex because the gradient and hessian of $||X - AB||_F$ is not positive semidefinite for all $X, A, B$.
	
	Consider the scalar case:
	$$
	\min (X-AB)^{2}=\min X^{2}-2 XAB +A^{2} B^{2}
	$$
	The Hessian of $\phi_{X}(A, B)=X^{2}-2 XAB-A^{2}B^{2}$ is
	$$
	\begin{array}{c}
	\nabla \phi_{x}(y, w)=\left[\begin{array}{c}
	2 y^{2} w-2 x y
	\end{array}\right] \\
	\nabla^{2} \phi_{x}(y, w)=\left[\begin{array}{cc}
	2 B^{2} & 4 A B-2 X \\
	4 AB-2 X& 2 A^{2}
	\end{array}\right]
	\end{array}
	$$
	The Hessian is not positive semidefinite for all $X, A, B$.
	
	\item Find the global optimum for this loss function. 
	\newline
	Using a truncated SVD, we find that the global optimum for this loss function as:
	$$  X = A \times \Sigma \times B$$
	The global optima can be found by taking the derivative of $$\sum_{i=1}^{n} \sum_{j=1}^{n} \left(\left(A \Sigma B\right)_{ij} - \left(AB\right)_{ij} \right)^2$$
	Upon doing so, we find that $$\Sigma = I \text{where $I$ is the $n \times n$ identity matrix}$$
\end{enumerate}
\end{solution}

\clearpage
\begin{solution}[Time spent: 2 hours]
	\begin{enumerate}
		\item Let $y = \lambda w$. Applying the chain rule to the given expression, we know:
\begin{align*}
\frac{dl(\lambda w)}{dw} = \frac{dl(y)}{dw} = \frac{dl(y)}{dy} \times \frac{dy}{dw} = \frac{dl(y)}{dy} \lambda = \frac{dl(\lambda w)}{d \lambda w} \lambda
\end{align*}
However, since we know that 
$$ l(w) = l(\lambda w) \implies \frac{dl(\lambda w)}{d \lambda w} = \frac{1}{\lambda} \times \frac{dl(\lambda w)}{dw} = \frac{1}{\lambda} \times \frac{dl(w)}{dw}$$
\qed
\item Given: $w^{t+1}=w^{t}-\eta \nabla \ell\left(w^{t}\right)$
\begin{align*}
w^{t+1}  &=  w^{t}-\eta \nabla \ell\left(w^{t}\right) \\
\frac{w^{t+1}}{||w^t||_2}  &=  \frac{w^{t}}{||w^t||_2} - \frac{\eta}{{||w^t||_2}} \nabla \ell\left(w^{t}\right) \\
\frac{w^{t+1}}{||w^t||_2}  &=  v^t - \frac{\eta}{{||w^t||^2_2}} \times ||w^t||_2 \nabla \ell\left(w^{t}\right) \\
\frac{w^{t+1}}{||w^t||_2}  &=  v^t - \frac{\eta}{{||w^t||^2_2}} \times ||w^t||_2 \nabla \ell\left(v^{t} \times ||w^t||_2\right) \\
\frac{w^{t+1}}{||w^t||_2}  &=  v^t - \frac{\eta}{{||w^t||^2_2}}  \nabla \ell\left(v^{t}\right) \\
\frac{w^{t+1}}{||w^{t+1}||_2} \times \frac{||w^{t+1}||_2}{||w^t||_2}  &=  v^t - \frac{\eta}{{||w^t||^2_2}}  \nabla \ell\left(v^{t}\right) \\
v^{t+1}  &=  \left[ v^t - \frac{\eta}{{||w^t||^2_2}}  \nabla \ell\left(v^{t}\right) \right] \times \frac{||w^t||_2}{||w^{t+1}||_2}\\
\end{align*}
\qed
\end{enumerate}

\end{solution}

\clearpage
\begin{solution}[Time spent: 0 hours]
	N / A
\end{solution}


\begin{solution}[Time spent: 7 hours]
\begin{enumerate}
	\item After training.
	\begin{figure}[H]
		\includegraphics[scale=.5]{images/q5a_training}
		\centering
		\caption{Training Loss}
	\end{figure}
	\begin{figure}[H]
	\includegraphics[scale=.5]{images/q5a_accuracy}
	\centering
	\caption{Validation Accuracy}
\end{figure}
	\item  Gradient plots
	\begin{figure}[H]
	\includegraphics[scale=.5]{images/q5b_correct_grad}
	\centering
	\caption{Visualization of gradients for correct predictions}
\end{figure}	
	\begin{figure}[H]
	\includegraphics[scale=.5]{images/q5b_wrong_grad}
	\centering
	\caption{Visualization of gradients for incorrect predictions}
\end{figure}
Loss on perturbed images.	
\begin{figure}[H]
\includegraphics[scale=.8]{images/perturbed_loss}
\centering
\caption{5-step-attack loss }
\end{figure}
\item
1-step perturbed images.
\begin{figure}[H]
	\includegraphics[scale=.8]{images/1step-accuracy}
	\centering
	\caption{Accuracy of model on 1-step perturbed images}
\end{figure}
How does this compare against the accuracy on the clean validation set
\newline
The accuracy is much much lower and more noisy. Validation on clean set was in the high 80\%.

\end{enumerate}
\end{solution}

\clearpage

\begin{solution}[Time spent: 3 hours]
\begin{enumerate}
	\item The back-propagation gradient for $w_z$ if the loss function is only a function of the hidden vector at time $T$.
	\begin{align*}
	\frac{dl(z^t)}{dw_z} &= \frac{dl(z^t)}{dz^t} \times  \frac{dz^t}{dw_z} \\
	\frac{dl(z^t)}{dw_z} &= \frac{dl(z^t)}{dz^t} \times \frac{dz^t}{dz^{t-1}} \times \frac{dz^{t-1}}{dz^{t-2}} \hdots \times \frac{dz^1}{dw_z}\\
	\frac{dl(z^t)}{dw_z} &= \frac{dl(z^t)}{dz^t} \times w_z^t \times z^0
	\end{align*}	
	Compute the conditions on the weight matrix $w_z^t$ under which the gradient explodes and vanishes.
	\newline
In the above equation, $w_z^t$ is exponentiated such that it explodes when $w_z^t > 1$ and vanishes when $w_z^t < 1$.

\item How does the nonlinearity affect the exploding or vanishing gradients? 
\newline
Nonlinearities or activation functions affect the exploding and vanishing gradients in the following way:
$$\frac{dz^t}{dz^{t-1}} = f(d \sigma) $$
That is, it becomes a function of the derivative of the activation function. For nonlinearities whose derivatives tend to evaluate to $< 1$, e.g. \verb|sigmoid|, \verb|tanh|, will lead to vanishing gradients. 
\newline
Which nonlinearities are well-suited for training RNNs?
\newline
Activation functions / nonlinearities like \verb|ReLU| whose gradient is fixed at 1 is much more well suited for the task of avoiding exploding and vanishing gradients.
\item How to protect the weights $w_z^{\verb|new|}$ from blowing up even if the gradient explodes
\newline We can apply functions which preserve the sign of the gradient while ensuring the value does not blow up. An example of this transformation:
$$\sigma\left(\frac{dl}{dw_z} - \frac{1}{2} \right) \text{where $\sigma$ is the sigmoid function}$$
\newline 
Can you modify the updates to handle the vanishing gradient problem?
\newline To handle vanishing gradients, we also need to lower bound the values of the gradient update.
\item Explain how the LSTM solves the problem of vanishing gradients?
\end{enumerate}
The long term dependencies and relations are encoded in the cell state vectors and itâ€™s the cell state derivative that can prevent the \verb|LSTM| gradients from vanishing. 

The presence of the forget gate, along with the additive property of the cell state gradients, enables the network to update parameters in such a way that the different sub gradients do not necessarily agree and behave in a similar manner, making it less likely that all of the gradients vanish, thus preventing the vanishing gradient problem.

\end{solution}

\end{document}