{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PS0-Problem 7.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8N5cnmT1zp3E",
        "colab_type": "text"
      },
      "source": [
        "The colab notebook is accessible at: https://colab.research.google.com/drive/1ejr7tjkDR_5oHMeQ1NrdnJ-EvDJSmCT7?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6YcdT1-PVIw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "63a06ac0-e536-4669-fea1-f27d14f7fee7"
      },
      "source": [
        "from sklearn.datasets import load_boston \n",
        "ds = load_boston()\n",
        "\n",
        "# explore the dataset\n",
        "print(ds.keys())\n",
        "print(ds.DESCR)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n",
            ".. _boston_dataset:\n",
            "\n",
            "Boston house prices dataset\n",
            "---------------------------\n",
            "\n",
            "**Data Set Characteristics:**  \n",
            "\n",
            "    :Number of Instances: 506 \n",
            "\n",
            "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
            "\n",
            "    :Attribute Information (in order):\n",
            "        - CRIM     per capita crime rate by town\n",
            "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
            "        - INDUS    proportion of non-retail business acres per town\n",
            "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
            "        - NOX      nitric oxides concentration (parts per 10 million)\n",
            "        - RM       average number of rooms per dwelling\n",
            "        - AGE      proportion of owner-occupied units built prior to 1940\n",
            "        - DIS      weighted distances to five Boston employment centres\n",
            "        - RAD      index of accessibility to radial highways\n",
            "        - TAX      full-value property-tax rate per $10,000\n",
            "        - PTRATIO  pupil-teacher ratio by town\n",
            "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
            "        - LSTAT    % lower status of the population\n",
            "        - MEDV     Median value of owner-occupied homes in $1000's\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
            "\n",
            "This is a copy of UCI ML housing dataset.\n",
            "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
            "\n",
            "\n",
            "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
            "\n",
            "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
            "prices and the demand for clean air', J. Environ. Economics & Management,\n",
            "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
            "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
            "pages 244-261 of the latter.\n",
            "\n",
            "The Boston house-price data has been used in many machine learning papers that address regression\n",
            "problems.   \n",
            "     \n",
            ".. topic:: References\n",
            "\n",
            "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
            "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYs_1tpnQDQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_DG0sF1RvYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = ds.data\n",
        "Y = ds.target\n",
        "m, n = X.shape\n",
        "\n",
        "# Add a column of 1s to the dataset to account for the bias term\n",
        "X = np.concatenate([X, np.ones((m, 1))], axis=1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GeMcVzmZRcPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def solve_weight(X, Y):\n",
        "    \"\"\"\n",
        "      Solve for the weight W using \n",
        "      the analytical formula W = (X^T X)^{-1} X^T Y\n",
        "    \"\"\"\n",
        "    Y = Y.reshape(-1, 1)\n",
        "    w = (np.linalg.inv(X.T @ X) @ X.T) @ Y\n",
        "    return w\n",
        "\n",
        "\n",
        "def get_split(X, Y, seed):\n",
        "    \"\"\"\n",
        "      Split the dataset into train and validation (80%/20%) sets\n",
        "    \"\"\"\n",
        "    return train_test_split(X, Y, test_size=0.20, random_state=seed)\n",
        "\n",
        "\n",
        "def find_residual(y_pred, y_true):\n",
        "    \"\"\"\n",
        "      Computes the residual using a function from sklearn.\n",
        "        * Note that the mean_squared_error function and the definition \n",
        "          of the residual in HW-0 differs by a factor of 2 \n",
        "    \"\"\"\n",
        "    return 0.5 * mean_squared_error(y_true, y_pred)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbpHbDKvRx66",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "25822387-9121-40e3-8873-6863355cba67"
      },
      "source": [
        "random_seeds = [0, 1000, 4204]\n",
        "\n",
        "train_res, val_res = [], []\n",
        "\n",
        "for seed in random_seeds:\n",
        "    # Split data and solve for weights\n",
        "    X_train, X_val, y_train, y_val = get_split(X, Y, seed)\n",
        "    W = solve_weight(X_train, y_train)\n",
        "   \n",
        "    # Make predictions based on learnt weight vector\n",
        "    y_train_pred = X_train @ W\n",
        "    y_val_pred = X_val @ W\n",
        "\n",
        "    # Store residuals\n",
        "    train_res.append(find_residual(y_train_pred, y_train))\n",
        "    val_res.append(find_residual(y_val_pred, y_val))\n",
        "\n",
        "print(\"Training residuals\")\n",
        "print(train_res)\n",
        "\n",
        "print(\"Validation residuals\")\n",
        "print(val_res)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training residuals\n",
            "[9.663235101792862, 10.585926141112633, 11.47943598442984]\n",
            "Validation residuals\n",
            "[16.724489998843257, 13.041182673583272, 9.462872045433993]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfE6Mx33SCig",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "48cb897f-db4d-429a-98f5-05693527ded2"
      },
      "source": [
        "# Some stats\n",
        "print(\"Mean and standard deviation of residuals on train data\")\n",
        "print(np.mean(train_res), np.std(train_res))\n",
        "print(\"Mean and standard deviation of residuals on validation data\")\n",
        "print(np.mean(val_res), np.std(val_res))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean and standard deviation of residuals on train data\n",
            "10.576199075778446 0.7414928066060814\n",
            "Mean and standard deviation of residuals on validation data\n",
            "13.076181572620174 2.9646464114650932\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}