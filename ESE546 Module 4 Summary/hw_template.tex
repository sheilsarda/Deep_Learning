\documentclass[11pt, reqno, letterpaper, twoside]{amsart}
\linespread{1.2}
\usepackage[margin=1.25in]{geometry}

\usepackage{amssymb, bm, mathtools}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage[pdftex, xetex]{graphicx}
\usepackage{enumerate, setspace}
\usepackage{float, colortbl, tabularx, longtable, multirow, subcaption, environ, wrapfig, textcomp, booktabs}
\usepackage{pgf, tikz, framed}
\usepackage[normalem]{ulem}
\usetikzlibrary{arrows,positioning,automata,shadows,fit,shapes}
\usepackage[english]{babel}
% \usepackage[table,xcdraw]{xcolor}
\usepackage{microtype}
\microtypecontext{spacing=nonfrench}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\theoremstyle{definition}
\newtheorem{solution}[theorem]{Solution}

\usepackage{times}
\title{ESE 546, Fall 2020\\[0.1in]
Module 4 Summary}
\author{
Sheil Sarda [sheil@seas]\\
}

\begin{document}
\maketitle
Lecture outlines and key takeaways for Module 4 \\
\textbf{Shape of the energy landscape of neural networks (Chapter 12) }
	\begin{enumerate}
		\item Introduction
		\item Deep Linear Networks
		\begin{enumerate}
			\item Least squares solution
			\item Projection of a vector onto a matrix
			\item Back to deep linear networks
			\item Critical points of $B$ if $A$ is fixed
			\item Critical points of $A$ if $B$ is fixed
			\item Critical points of $(A, B)$ 
			\item If $W$ is a critical point then it can be written as a projection of the least squares solution on the subspace spanned by some $p$ eigenvectors of $\Sigma$		
			\item If $W$ is the global minimum for a two-layer network then it is a projection of the solution for a single-layer network onto the subspace spanned by the top $p$ eigenvectors of $\Sigma$
			\item There are exponentially many saddle points for a two-layer network
			\item No local minima in a deep linear network, all minima are global and not unique
			\item All the previous results are true for multi-layer linear networks			
		\end{enumerate}
			\item Extending the picture to deep networks 
	\end{enumerate}
Key takeaways
\begin{itemize}
\item Hello \\
\end{itemize}	
\textbf{Generalization performance of machine learning models (Chapter 13) }
	\begin{enumerate}
		\item The PAC-Learning Model
		\begin{enumerate}
			\item PAC-learnable hypothesis class
			\item Learning Monotone Boolean Formulae
		\end{enumerate}		
		\item Concentration of Measure
		\begin{enumerate}
			\item Union Bound (or Boole's Inequality)
			\item Weak Law, Strong Law and Central Limit Theorem
			\item Chernoff Bound
		\end{enumerate}	
		\item Uniform Convegence 
		\item Vapnik-Chernovenkis (VC) dimension
		\begin{enumerate}
			\item Shattering a set of inputs
			\item Bounds on the VC-dimension of deep neural networks
		\end{enumerate}	
	\end{enumerate}
Key takeaways
\begin{itemize}
\item Hello \\
\end{itemize}	
\textbf{Variational Inference (Chapter 14)} 
	\begin{enumerate}
		\item The model
		\item Some technical basics
		\begin{enumerate}
			\item Variational calculus
			\begin{enumerate}
				\item Picking the domain and objective in variational optimization
				\item Choosing a functional to measure the distance between $q$ and $p$
			\end{enumerate}
			\item Laplace approximation
			\item Digging deeper into KL-Divergence
		\end{enumerate}		
		\item Evidence Lower Bound (ELBO)
		\begin{enumerate}
			\item Parameterizing ELBO
		\end{enumerate}	
		\item Gradient of the ELBO
		\begin{enumerate}
			\item The Reparameterization Trick
			\item Score function estimator of the gradient
			\item Gradient of the remaining terms in ELBO
		\end{enumerate}	
		\item Some comments 	
	\end{enumerate}
Key takeaways
\begin{itemize}
\item Hello \\
\end{itemize}	
\textbf{Generative Adversarial Networks (Chapter 15)} 
	\begin{enumerate}
		\item Two-sample tests and Discriminators
		\item Building the Discriminator in a GAN
		\item Building the Generator of a GAN
		\item Putting the discriminator and generator together
		\begin{enumerate}
			\item Training a GAN
			\item Solving min-max problems is difficult
			\item A harsh discriminator inhibits the training of the generator
		\end{enumerate}	
		\item How to perform valiation for a GAN
		\item The zoo of GANs
	\end{enumerate}
Key takeaways
\begin{itemize}
\item Hello \\
\end{itemize}



Table of lecture and recitation topics:
% Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% If you use beamer only pass "xcolor=table" option, i.e. \documentclass[xcolor=table]{beamer}
\begin{table}[H]
\centering
\centering
\begin{tabular}{|l|l|}
\hline
\rowcolor[HTML]{000000} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{000000}{\color[HTML]{FFFFFF} \textbf{Lecture}}} & \multicolumn{1}{c|}{\cellcolor[HTML]{000000}{\color[HTML]{FFFFFF} \textbf{Topic}}} \\ \hline
21     & SGD III, Background on Principal Component Analysis           \\ \hline
Rec 12 & Generalization bound using uniform convergence                \\ \hline
22     & Deep Linear networks                                          \\ \hline
Rec 13 & Vignette: Deep reinforcement learning                         \\ \hline
23     & Background: Information theory and Variational Inference      \\ \hline
24     & Auto-Encoders, ELBO I                                         \\ \hline
Rec 14 & Recap of info theory, variational auto-encoders               \\ \hline
25     & ELBO II                                                       \\ \hline
26     & Bayesian neural networks                                      \\ \hline
27     & Generative Adversarial Networks, Recap of post-midterm topics \\ \hline
\end{tabular}
\end{table}
\end{document}