{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# data = pd.read_csv('book-war-and-peace.txt', header = None, sep=\" \", error_bad_lines=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = open('book-war-and-peace.txt', \"r\")\n",
    "# print(txt.readline())\n",
    "# print(txt.read(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_chars = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in txt:\n",
    "    for char in line:\n",
    "        book_chars.append(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C', 'H', 'A', 'P', 'T', 'E', 'R', ' ', 'I', '\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_chars[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements: {'/', 'w', '1', '6', '4', 'a', 'X', 'z', 'E', '\\n', '\"', 'R', 'h', \"'\", 'S', 'W', 'D', 'n', '9', 't', 'd', '.', 'f', 'ä', 'y', 'Y', 'i', 'l', '!', '7', 'Q', 'P', 'j', 'K', ':', 'v', '0', 'r', 'ê', '*', 'p', 'b', 'q', 'I', 'e', '=', 'F', 'm', 'o', '?', '-', ')', '8', 'L', 'N', '3', ';', ',', 'J', 'g', 'M', 'T', 'C', 'c', '5', 'u', 'O', 'A', 'H', 'é', 's', 'U', 'B', ' ', 'V', 'k', '2', 'x', 'à', 'Z', '(', 'G'}\n",
      "Set size: 82\n"
     ]
    }
   ],
   "source": [
    "s=set(book_chars)\n",
    "print('Elements:',s)\n",
    "print('Set size:', len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for char in s:\n",
    "    book_dict[char] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3202303"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(book_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_dict[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_chars = np.array(book_chars)\n",
    "dict_size = len(s)\n",
    "seq_len = 1\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(sequence, dict_size, \n",
    "                   seq_len, batch_size,\n",
    "                      num_batches, book_dict):\n",
    "    # Creating a multi-dimensional array of zeros with the \n",
    "    # desired output shape\n",
    "    features = np.zeros((batch_size, seq_len, dict_size), \n",
    "                        dtype=np.float32)\n",
    "\n",
    "    # Replacing the 0 at the relevant character index \n",
    "    # with a 1 to represent that character\n",
    "    for i in range(batch_size):\n",
    "        for u in range(seq_len):\n",
    "            char_ix = (num_batches - 1) * batch_size + i\n",
    "            features[i, u, book_dict[sequence[char_ix]]] = 1\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encode(sequence, batch_size, char_dict, num_batches):\n",
    "    batch_list = []\n",
    "    for x in range(batch_size):\n",
    "        char_idx = (num_batches - 1) * batch_size + x\n",
    "        char_val = char_dict[sequence[char_idx]]\n",
    "        batch_list.append(char_val)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "features = one_hot_encode(book_chars, dict_size, seq_len, batch_size, 2, book_dict)\n",
    "print(features[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train Split using 70-30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3202303 2241612 960691\n"
     ]
    }
   ],
   "source": [
    "train_char_len = (int) (0.7 * len(book_chars))\n",
    "test_char_len = len(book_chars) - train_char_len\n",
    "print(len(book_chars), train_char_len, test_char_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89664 38427 128091\n"
     ]
    }
   ],
   "source": [
    "train_batches = (int) (train_char_len / batch_size)\n",
    "test_batches = (int) (test_char_len / batch_size)\n",
    "print(train_batches, test_batches,test_batches + train_batches )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for i in range(train_batches):\n",
    "    minibatch = one_hot_encode(book_chars, dict_size, seq_len, batch_size, i, book_dict)\n",
    "    X_train.append(torch.Tensor(minibatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "for i in range(train_batches):\n",
    "    Y_train.append(torch.LongTensor(target_encode(book_chars[1:], batch_size, book_dict, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89664\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "89664\n",
      "tensor([26, 63, 12, 73,  1, 44, 73,  5, 37, 44, 73, 17, 48, 19,  9, 63, 48, 17,\n",
      "        70, 63, 26, 48, 65, 70, 21])\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(X_train[-1][0])\n",
    "print(len(Y_train))\n",
    "print(Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = []\n",
    "for i in range(train_batches, train_batches + test_batches):\n",
    "    minibatch = one_hot_encode(book_chars, dict_size, seq_len, batch_size, i, book_dict)\n",
    "    X_test.append(torch.Tensor(minibatch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = []\n",
    "for i in range(train_batches, train_batches + test_batches):\n",
    "    Y_test.append(torch.LongTensor(target_encode(book_chars[1:], batch_size, book_dict, i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38427\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "38427\n",
      "tensor([12, 44, 73, 22, 65, 37, 17, 26, 19, 65, 37, 44, 73,  1,  5, 70,  9, 41,\n",
      "        44, 26, 17, 59, 73, 63,  5])\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))\n",
    "print(X_test[-1][0])\n",
    "print(len(Y_test))\n",
    "print(Y_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_book():\n",
    "    char_vals = []\n",
    "    for c in book_chars:\n",
    "        char_vals.append(book_dict[c])\n",
    "    return char_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.66 s, sys: 62.5 ms, total: 2.72 s\n",
      "Wall time: 3.48 s\n",
      "3202303\n"
     ]
    }
   ],
   "source": [
    "%time vals = convert_book()\n",
    "print(len(vals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, no_layers = 1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.no_layers = no_layers\n",
    "        \n",
    "        self.rnn_layer = nn.RNN(self.input_dim, \n",
    "                        self.hidden_dim, self.no_layers,\n",
    "                                batch_first = True)\n",
    "        self.linear_out = nn.Linear(self.hidden_dim, self.output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = torch.zeros(self.no_layers, batch_size, self.hidden_dim).requires_grad_()\n",
    "        out, hidden = self.rnn_layer(x, hidden.detach())\n",
    "        out = out.view(-1, self.hidden_dim)\n",
    "        out = self.linear_out(out)\n",
    "        out = self.softmax(out)\n",
    "        return out, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = len(s)\n",
    "hidden_size = 128\n",
    "output_size = len(s)\n",
    "rnn = RNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "lr = 1e-3\n",
    "# Loss, Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 82])\n",
      "tensor(4.4124, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "out, hidden = rnn(X_train[0])\n",
    "loss = criterion(output, Y_train[0])\n",
    "# print(out[:2])\n",
    "print(out.size())\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([26, 63, 12, 73,  1, 44, 73,  5, 37, 44, 73, 17, 48, 19,  9, 63, 48, 17,\n",
       "         70, 63, 26, 48, 65, 70, 21]),\n",
       " tensor([68, 67, 31, 61,  8, 11, 73, 43,  9,  9, 10, 15, 44, 27, 27, 57, 73, 31,\n",
       "         37, 26, 17, 63, 44, 57, 73])]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs+1):\n",
    "    for x, target in zip(X_train, Y_train):\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output, hidden = rnn(x)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        if epoch%5 == 0:\n",
    "            print('Epoch: {}/{}.............'.format(epoch, epochs), end=' ')\n",
    "            print(\"Loss: {:.4f}\".format(loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
