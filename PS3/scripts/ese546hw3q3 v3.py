# -*- coding: utf-8 -*-
"""ESE546HW3Q3 v3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dULjW_jzzR5pAcxR_faCg5C5xQGMJcvv

## Import libraries
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

# import pandas as pd
# data = pd.read_csv('book-war-and-peace.txt', header = None, sep=" ", error_bad_lines=False);

txt = open('book-war-and-peace.txt', "r")
# print(txt.readline())
# print(txt.read(10))

book_chars = []

for line in txt:
    for char in line:
        book_chars.append(char)

txt.close()

# book_chars[:10]
# book_chars = book_chars[:100000]

s=set(book_chars)
print('Elements:',s)
print('Set size:', len(s))

book_dict = {}

i = 0
for char in s:
    book_dict[char] = i
    i += 1

len(book_chars)

book_dict["a"]



"""## One-hot encoding"""

book_chars = np.array(book_chars)
dict_size = len(s)
seq_len = 1
batch_size = 25

def one_hot_encode(sequence, dict_size, 
                   seq_len, batch_size,
                      num_batches, book_dict):
    # Creating a multi-dimensional array of zeros with the 
    # desired output shape
    features = np.zeros((batch_size, seq_len, dict_size), 
                        dtype=np.float32)

    # Replacing the 0 at the relevant character index 
    # with a 1 to represent that character
    for i in range(batch_size):
        for u in range(seq_len):
            char_ix = (num_batches - 1) * batch_size + i
            features[i, u, book_dict[sequence[char_ix]]] = 1
    return features

def target_encode(sequence, batch_size, char_dict, num_batches):
    batch_list = []
    for x in range(batch_size):
        char_idx = (num_batches - 1) * batch_size + x
        char_val = char_dict[sequence[char_idx]]
        batch_list.append(char_val)
    return batch_list

features = one_hot_encode(book_chars, dict_size, seq_len, batch_size, 2, book_dict)
print(features[-1])



"""## Test Train Split using 70-30"""

train_char_len = (int) (0.7 * len(book_chars))
test_char_len = len(book_chars) - train_char_len
print(len(book_chars), train_char_len, test_char_len)

train_batches = (int) (train_char_len / batch_size)
test_batches = (int) (test_char_len / batch_size)
print(train_batches, test_batches,test_batches + train_batches )



X_train = []
for i in range(train_batches):
    minibatch = one_hot_encode(book_chars, dict_size, seq_len, batch_size, i, book_dict)
    X_train.append(torch.Tensor(minibatch))

Y_train = []
for i in range(train_batches):
    Y_train.append(torch.LongTensor(target_encode(book_chars[1:], batch_size, book_dict, i)))

print(len(X_train))
print(X_train[-1][0])
print(len(Y_train))
print(Y_train[0])



X_test = []
for i in range(train_batches, train_batches + test_batches):
    minibatch = one_hot_encode(book_chars, dict_size, seq_len, batch_size, i, book_dict)
    X_test.append(torch.Tensor(minibatch))

Y_test = []
for i in range(train_batches, train_batches + test_batches):
    Y_test.append(torch.LongTensor(target_encode(book_chars[1:], batch_size, book_dict, i)))

print(len(X_test))
print(X_test[-1][0])
print(len(Y_test))
print(Y_test[1])

X_train = X_train[:40000]
Y_train = Y_train[:40000]
X_test = X_test[:1200]
Y_test = Y_test[:1200]

def convert_book():
    char_vals = []
    for c in book_chars:
        char_vals.append(book_dict[c])
    return char_vals

# Commented out IPython magic to ensure Python compatibility.
# %time vals = convert_book()
print(len(vals))

"""## Model definition"""

class RNN(nn.Module):
    
    def __init__(self, input_dim, hidden_dim, output_dim, no_layers = 1):
        super(RNN, self).__init__()
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.output_dim = output_dim
        self.no_layers = no_layers
        
        self.rnn_layer = nn.RNN(self.input_dim, 
                        self.hidden_dim, self.no_layers,
                                batch_first = True,
                                nonlinearity='tanh')
        self.linear_out = nn.Linear(self.hidden_dim, self.output_dim)
        self.softmax = nn.LogSoftmax(dim = 1)
        
    def forward(self, x):
        batch_size = x.size(0)
        hidden = torch.zeros(self.no_layers, batch_size, self.hidden_dim).requires_grad_()
        out, hidden = self.rnn_layer(x, hidden.detach())
        # out = out.view(-1, self.hidden_dim)
        # out = self.linear_out(out)
        # out = self.softmax(out)
        out = out.view(batch_size, len(s))
        return out, hidden

input_size = len(s)
hidden_size = len(s) # 128
output_size = len(s)
rnn = RNN(input_size, hidden_size, output_size)

epochs = 100
lr = 1e-3
# Loss, Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(rnn.parameters(), lr = lr)

"""## Training Run"""

out, hidden = rnn(X_train[0])
loss = criterion(out, Y_train[0])
# print(out[:2])
print(out.size())
print(loss)

print(len(Y_test[:2]))
len(Y_test)

device = torch.device("cuda")

train_loss = []
val_error = []
counter = 0
# rnn.train()

for epoch in range(1, epochs+1):
    for x, target in zip(X_train, Y_train):
        rnn.train()
        rnn.zero_grad()
        optimizer.zero_grad()
        x.to(device)
        target.to(device)
        output, hidden = rnn(x)
               
        loss = criterion(output, target)
        
        loss.backward() 
        nn.utils.clip_grad_norm_(rnn.parameters(), 1)
        optimizer.step() 

        
        if(counter % 1000) == 0:

          print("Iteration: {}, Loss: {}".format(counter, loss))
          rnn.eval()
          train_loss.append(loss.item())
          correct = 0
          total = 0
          with torch.no_grad():
            rnn.eval()
            for x_test, y_test in zip(X_test[:10], Y_test[:10]):
              out, hidden = rnn(x_test)
              total += batch_size

              val = criterion(out, y_test)
              val_error.append(val)

              _, pred = torch.max(out.data, 1)
              correct += (pred == y_test).sum()
        counter += 1


    if epoch%5 == 0:
        print('Epoch: {}/{}.............'.format(epoch, epochs), end=' ')
        print("Loss: {:.4f}".format(loss.item()))

from matplotlib import pyplot as plt

plt.plot(np.arange(len(val_error)), val_error)

plt.plot(np.arange(len(train_loss)), train_loss)

