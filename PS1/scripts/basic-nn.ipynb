{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision as thv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28]) 60000\n"
     ]
    }
   ],
   "source": [
    "# Doanload Data\n",
    "data_exists = os.path.isdir(\"./MNIST\")\n",
    "train = thv.datasets.MNIST('./', download=data_exists, train=True)\n",
    "val = thv.datasets.MNIST('./', download=data_exists, train=False)\n",
    "\n",
    "print(train.data.shape, len(train.targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer_t:\n",
    "    def backward_check(self, dh_l1):\n",
    "        # PS: since the matrix dw is really sparse, it's good to try and visualize it\n",
    "        # for picking i,j values or try many different values\n",
    "        i, j = 7, 432\n",
    "\n",
    "        k = np.argwhere(dh_l1 == 1)[0, 1]\n",
    "        epsilon_ij = np.random.normal(0, 1)\n",
    "        epsilon = np.zeros_like(self.w)\n",
    "        epsilon[i, j] = epsilon_ij\n",
    "\n",
    "        dw_ij_num = np.matmul((self.w + epsilon), self.hl.T)[k, 0] - np.matmul((self.w - epsilon), self.hl.T)[k, 0]\n",
    "        dw_ij_den = 2 * epsilon_ij\n",
    "\n",
    "        dw_ij = dw_ij_num / dw_ij_den\n",
    "\n",
    "        return dw_ij, self.dw[i,j]\n",
    "    \n",
    "    def backward_check_h(self, dh_l1, i=7):\n",
    "        # PS: since the matrix hl is really sparse, it's good to try and visualize it\n",
    "        # for picking i values or try many different values\n",
    "        k = np.argwhere(dh_l1 == 1)[0, 1]\n",
    "        \n",
    "        epsilon_i = np.random.normal(0, 1)\n",
    "        epsilon = np.zeros_like(self.hl)\n",
    "        epsilon[0, i] = epsilon_i\n",
    "\n",
    "        dw_i_num = np.matmul((self.hl + epsilon), self.hl.T) - np.matmul((self.hl - epsilon), self.hl.T)\n",
    "        dw_i_den = 2 * epsilon_i\n",
    "\n",
    "        dw_i = dw_i_num / dw_i_den\n",
    "        \n",
    "        return dw_i, self.hl[0, k]\n",
    "    \n",
    "    def backward_check_b(self, dh_l1, i=7):\n",
    "        # PS: since the matrix hl is really sparse, it's good to try and visualize it\n",
    "        # for picking i values or try many different valuesgo\n",
    "        k = np.argwhere(dh_l1 == 1)[0, 1]\n",
    "        \n",
    "        epsilon_i = np.random.normal(0, 1)\n",
    "        epsilon = np.zeros_like(self.b)\n",
    "        epsilon[i] = epsilon_i\n",
    "\n",
    "        dw_i_num = np.matmul((self.b + epsilon), self.hl)[k,0] - np.matmul((self.b - epsilon), self.hl)[k,0]\n",
    "        dw_i_den = 2 * epsilon_i\n",
    "\n",
    "        dw_i = dw_i_num / dw_i_den\n",
    "        \n",
    "        return dw_i, self.hl[0, k]\n",
    "\n",
    "    def zero_grad(self):\n",
    "        # useful to delete the stored backprop gradients of the\n",
    "        # previous mini-batch before you start a new mini-batch\n",
    "        # --- only used in linear layer\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_t(layer_t):\n",
    "    def __init__(self):\n",
    "        # input size\n",
    "        self.A = 784 \n",
    "        \n",
    "        # class count\n",
    "        self.C = 10\n",
    "        \n",
    "        # Fix seed\n",
    "        np.random.seed(546)\n",
    "        \n",
    "        # Define normalized w, and b\n",
    "        self.w = np.random.normal(loc=0, scale=1, size=(self.C, self.A))\n",
    "        self.w = self.w / np.linalg.norm(self.w)\n",
    "\n",
    "        self.b = np.random.normal(loc=0, scale=1, size=(self.C, 1))\n",
    "        self.b = self.b / np.linalg.norm(self.b)\n",
    "        \n",
    "        # Define placeholder gradients to be filled in backward step\n",
    "        self.dw = np.zeros_like(self.w)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "        \n",
    "\n",
    "    def forward(self, h_l):\n",
    "        h_l1 = np.matmul(h_l, np.transpose(self.w)) + self.b.T\n",
    "\n",
    "        # cache hË†l in forward because we will need it to compute\n",
    "        # dw in backward\n",
    "        self.hl = h_l\n",
    "        \n",
    "        return h_l1\n",
    "\n",
    "\n",
    "    def backward(self, dh_l1):\n",
    "        dh_l = np.matmul(dh_l1, self.w)\n",
    "        dw = np.matmul(dh_l1.T, self.hl)\n",
    "        db = np.matmul(dh_l1.T, np.ones([self.hl.shape[0], 1]))\n",
    "\n",
    "        self.dw, self.db = dw, db\n",
    "    \n",
    "        # notice that there is no need to cache dh_{l+1}\n",
    "        return dh_l\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        # useful to delete the stored backprop gradients of the\n",
    "        # previous mini-batch before you start a new mini-batch\n",
    "        self.dw, self.db = 0 * self.dw, 0 * self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class relu_t(layer_t):\n",
    "    def forward(self, h_l):\n",
    "        return np.maximum(0, h_l)\n",
    "    \n",
    "    def backward(self, dh_l1):\n",
    "        x = np.array(dh_l1, copy=True)\n",
    "        x[x <= 0] = 0\n",
    "        x[x > 0] = 1\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_cross_entropy_t(layer_t):\n",
    "    def __init__(self):\n",
    "        # no parameters, nothing to initialize\n",
    "        self.h_l1 = None\n",
    "        self.y = None\n",
    "\n",
    "    def forward(self, h_l, y):\n",
    "        expd = np.exp(h_l)\n",
    "        h_l1 = expd / np.sum(expd, axis=1,keepdims=True) \n",
    "        \n",
    "        self.h_l1 = h_l1\n",
    "        self.y = y\n",
    "       \n",
    "        # compute average loss ell(y) over a mini-batch\n",
    "        BATCH_SIZE = h_l1.shape[0]\n",
    "        ell = np.sum(-np.log(h_l1[range(BATCH_SIZE), self.y])) / BATCH_SIZE\n",
    "        \n",
    "        # compute the error of predictions\n",
    "        error = np.sum(np.not_equal(self.y, np.argmax(h_l1, axis=1)))/BATCH_SIZE\n",
    "        \n",
    "        return ell, error\n",
    "\n",
    "    def backward(self):\n",
    "        # as we saw in the notes, the backprop input to the\n",
    "        # loss layer is 1, so this function does not take any\n",
    "        # arguments\n",
    "        \n",
    "        B = self.y.shape[0] # batch size\n",
    "        self.h_l1[range(B), self.y] -=1\n",
    "        \n",
    "        return self.h_l1/B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample(dataset, num_samples, num_classes):\n",
    "    # Get subsample by sampling\n",
    "    num_samples_per_class = int(num_samples/num_classes)\n",
    "    dataset = np.array(list(map(lambda elt: (np.array(elt[0]).flatten(),elt[1]), dataset)))\n",
    "    X, y = map(lambda x: np.array(list(x)),zip(*dataset))\n",
    "    subsamplesX, subsamplesY = None, None\n",
    "\n",
    "    for c in range(num_classes):\n",
    "        # Get all X's where label == c\n",
    "        mask = np.argwhere(y == c)\n",
    "        class_X, class_Y = X[mask], y[mask]\n",
    "        class_X = class_X.reshape((class_X.shape[0], X.shape[1]))\n",
    "        \n",
    "        # Get all X's where label == c, inds = index array\n",
    "        inds = np.random.randint(0, high=class_Y.shape[0], size=num_samples_per_class)\n",
    "        subsamplesX = class_X[inds] if subsamplesX is None else np.concatenate([subsamplesX, class_X[inds]])\n",
    "        \n",
    "        # Convert Y to one-hot\n",
    "#         class_subsample = np.zeros((num_samples_per_class, num_classes))\n",
    "#         class_subsample[np.arange(num_samples_per_class), class_Y[inds]] = 1\n",
    "#         subsamplesY = class_subsample if subsamplesY is None else np.concatenate([subsamplesY,class_subsample])\n",
    "\n",
    "        # Get all Y's where label == c\n",
    "        subsamplesY = class_Y[inds] if subsamplesY is None else np.concatenate([subsamplesY, class_Y[inds]])\n",
    "    \n",
    "    # shuffle the arrays\n",
    "    A = subsamplesX.shape[1]\n",
    "    B = subsamplesX.shape[0]\n",
    "    shuf_inds = np.random.shuffle(np.array(range(B)))\n",
    "    return subsamplesX[shuf_inds].reshape((B,A)), subsamplesY[shuf_inds].reshape((num_samples,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(y_i):\n",
    "    hot_y = np.zeros((10,)) # C = 10\n",
    "    hot_y[y_i] = 1\n",
    "    return hot_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated gradient = -0.0, true gradient = 195.0\n"
     ]
    }
   ],
   "source": [
    "######## CODE FOR CHECKING GRADIENT MATCHING #############\n",
    "\n",
    "## THIS FAILS FOR SOME REASON ##\n",
    "\n",
    "l1 = linear_t()\n",
    "l2 = relu_t()\n",
    "l3 = softmax_cross_entropy_t()\n",
    "\n",
    "h1 = l1.forward(np.array([trainX[25064]]))\n",
    "h2 = l2.forward(h1)\n",
    "ell, error = l3.forward(h2, np.array([trainY[25064]]))\n",
    "\n",
    "\n",
    "dh2 = l3.backward()\n",
    "dh1 = l2.backward(dh2)\n",
    "dx = l1.backward(dh1)\n",
    "\n",
    "estimate_grad, true_grad = l1.backward_check(dh1)\n",
    "print('Estimated gradient = {}, true gradient = {}'.format(estimate_grad, true_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sheil/miniconda3/envs/TF/lib/python3.7/site-packages/ipykernel_launcher.py:4: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 784) (30000,)\n"
     ]
    }
   ],
   "source": [
    "trainX, trainY = subsample(dataset=train, num_samples=30000, num_classes=10)\n",
    "# valX, valY = subsample(dataset=val, num_samples=5000, num_classes=10)\n",
    "print(trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "##################################################################\n",
    "############### TRAINING THE NEURAL NETWORK ######################\n",
    "##################################################################\n",
    "##################################################################\n",
    "\n",
    "l1, l2, l3 = linear_t(), relu_t(), softmax_cross_entropy_t()\n",
    "net = [l1, l2, l3]\n",
    "\n",
    "# number of iterations\n",
    "iters = 50000\n",
    "# learning rate\n",
    "lr = 0.1\n",
    "\n",
    "# mini-batch size\n",
    "batch_size = 1\n",
    "\n",
    "# all training data size\n",
    "training_data_size = trainX.shape[0]\n",
    "\n",
    "loss = []\n",
    "# train for at least \"iters\" iterations\n",
    "for t in range(iters):\n",
    "    # 1. sample a mini-batch of size bb = 32\n",
    "    # each image in the mini-batch is chosen uniformly randomly from the # training dataset\n",
    "    inds = np.random.randint(0,high=training_data_size, size=batch_size)\n",
    "    x, y = trainX[inds], trainY[inds]\n",
    "    \n",
    "\n",
    "    # 2. zero gradient buffer\n",
    "    for l in net: \n",
    "        l.zero_grad()\n",
    "        \n",
    "    # 3. forward pass\n",
    "    h1 = l1.forward(x)\n",
    "    h2 = l2.forward(h1)\n",
    "    ell, error = l3.forward(h2, y)\n",
    "    \n",
    "#     print(f\"Given true class: {y} our network outputted probabilities: {l3.h_l1}, which is class: {np.argmax(l3.h_l1)}\")\n",
    "    \n",
    "    # 4. Backward pass\n",
    "    dh2 = l3.backward()\n",
    "    dh1 = l2.backward(dh2)\n",
    "    dx  = l1.backward(dh1)\n",
    "    \n",
    "#     print(f\"Our network fixed it by subtracting 1 from the true class and returning: {dh2}\")\n",
    "    \n",
    "#     print(f\"After ReLu layer we get:: {dh1}\")\n",
    "    \n",
    "    \n",
    "    # 5. gather backprop gradients\n",
    "    dw, db = l1.dw, l1.db\n",
    "    \n",
    "    # 6. print some quantities for logging and debugging\n",
    "#     print(f\"Iteration #{t} has average loss:{ell} and error: {error}\")\n",
    "#     print(t, np.linalg.norm(dw/l1.w), np.linalg.norm(db/l1.b))\n",
    "    \n",
    "    # 7. one step of SGD\n",
    "    l1.w = l1.w - lr*dw\n",
    "    l1.b = l1.b - lr*db\n",
    "    \n",
    "    loss.append(error)\n",
    "    \n",
    "    \n",
    "#     for debugging:\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
